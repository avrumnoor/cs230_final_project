{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing data extraction and aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook consists of two steps:\n",
    "\n",
    "1. Extraction of relevant sales price values of homes in the ZTRAX database[<sup>1</sup>](#fn1).\n",
    "2. Filtering of the data with some QA/QC algorithms and aggregation of the remaining building-level sales prices to an average \"price per sq ft.\" value for each grid cell in our POP sample.\n",
    "\n",
    "<span id=\"fn1\"> <sup>1</sup>Data provided by Zillow through the Zillow Transaction and Assessment Dataset (ZTRAX). More information on accessing the data can be found athttp://www.zillow.com/ztrax. The results and opinions are those of the author(s) and do not reflect the position of Zillow Group. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env variable MOSAIKS_HOME not defined; setting to: \"/shares/lab/Datasets/Remote_Sensing/replication\"\n",
      "If not desired, please reset os.environ[\"MOSAIKS_NAME\"]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime as dt\n",
    "from glob import glob\n",
    "from os.path import basename, dirname, isfile, join\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely as shp\n",
    "import shapely.geometry\n",
    "import shapely.vectorized\n",
    "from dask.distributed import Client, LocalCluster, progress\n",
    "from mosaiks import config as cfg\n",
    "from mosaiks.utils import io\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "app = \"housing\"\n",
    "cfg = io.get_filepaths(cfg, app)\n",
    "chousing = cfg.housing\n",
    "c_ext = chousing[\"data\"][\"ztrax\"]\n",
    "grid_str, _ = io.get_suffix(cfg, app)\n",
    "grid_path = join(cfg.grid_dir, \"grid_\" + grid_str + \".npz\")\n",
    "sample_path = join(cfg.grid_dir, \"{}.npz\".format(cfg.data_suffix))\n",
    "\n",
    "# location of raw ztrax data\n",
    "ztrax_dir_raw = join(cfg.data_dir, \"raw\", \"path\", \"to\", \"ztrax\", \"database\")\n",
    "\n",
    "# location of extracted ztrax data from step 1\n",
    "ztrax_dir_int = join(dirname(cfg.outcomes_fpath), \"states\")\n",
    "os.makedirs(ztrax_dir_int, exist_ok=True)\n",
    "\n",
    "# location of state shapefile\n",
    "state_path = join(\n",
    "    cfg.data_dir, \"raw\", \"shapefiles\", \"USA\", \"gadm_USA_shp\", \"gadm_USA_1.shp\"\n",
    ")\n",
    "\n",
    "# location of the 2 different databases in ZTRAX\n",
    "zillow_as_path = join(ztrax_dir_raw, \"Zillow_Assessor\")\n",
    "zillow_trans_path = join(ztrax_dir_raw, \"Zillow_Transaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_codes = {\n",
    "    \"WA\": \"53\",\n",
    "    \"DE\": \"10\",\n",
    "    \"DC\": \"11\",\n",
    "    \"WI\": \"55\",\n",
    "    \"WV\": \"54\",\n",
    "    \"HI\": \"15\",\n",
    "    \"FL\": \"12\",\n",
    "    \"WY\": \"56\",\n",
    "    \"PR\": \"72\",\n",
    "    \"NJ\": \"34\",\n",
    "    \"NM\": \"35\",\n",
    "    \"TX\": \"48\",\n",
    "    \"LA\": \"22\",\n",
    "    \"NC\": \"37\",\n",
    "    \"ND\": \"38\",\n",
    "    \"NE\": \"31\",\n",
    "    \"TN\": \"47\",\n",
    "    \"NY\": \"36\",\n",
    "    \"PA\": \"42\",\n",
    "    \"AK\": \"02\",\n",
    "    \"NV\": \"32\",\n",
    "    \"NH\": \"33\",\n",
    "    \"VA\": \"51\",\n",
    "    \"CO\": \"08\",\n",
    "    \"CA\": \"06\",\n",
    "    \"AL\": \"01\",\n",
    "    \"AR\": \"05\",\n",
    "    \"VT\": \"50\",\n",
    "    \"IL\": \"17\",\n",
    "    \"GA\": \"13\",\n",
    "    \"IN\": \"18\",\n",
    "    \"IA\": \"19\",\n",
    "    \"MA\": \"25\",\n",
    "    \"AZ\": \"04\",\n",
    "    \"ID\": \"16\",\n",
    "    \"CT\": \"09\",\n",
    "    \"ME\": \"23\",\n",
    "    \"MD\": \"24\",\n",
    "    \"OK\": \"40\",\n",
    "    \"OH\": \"39\",\n",
    "    \"UT\": \"49\",\n",
    "    \"MO\": \"29\",\n",
    "    \"MN\": \"27\",\n",
    "    \"MI\": \"26\",\n",
    "    \"RI\": \"44\",\n",
    "    \"KS\": \"20\",\n",
    "    \"MT\": \"30\",\n",
    "    \"MS\": \"28\",\n",
    "    \"SC\": \"45\",\n",
    "    \"KY\": \"21\",\n",
    "    \"OR\": \"41\",\n",
    "    \"SD\": \"46\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is extracted by state, and uses the [dask](https://docs.dask.org/en/latest/) package to process each state in parallel if possible. Dask is configurable such that, by default, this code should run on a single shared-memory machine, using as many cores are available. If you are running this on a multi-node cluster, you will need to do some additional setup to configure dask with whatever job scheduler you might have on the cluster.\n",
    "\n",
    "Here you will want to configure several things relevant to the machine you are working on and instantiate the dask scheduler that will run each process extracting data from the database.\n",
    "- `chunksize` is the number of rows of each ZTRAZX csv to process. Setting it too high will cause memory issues. Setting it too low will require too much I/O time.\n",
    "- `cluster_kwargs` consists of any `dask` kwargs you would like to pass to your Cluster instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:44886</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>100.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:44886' processes=1 cores=4>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize = 1000000\n",
    "cluster_kwargs = {\"n_workers\": 1, \"threads_per_worker\": 4, \"memory_limit\": \"100G\"}\n",
    "\n",
    "cluster = LocalCluster(**cluster_kwargs)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Price/sq ft. extraction and aggregation\n",
    "\n",
    "This process occurs in two steps. In the first, we filter to the sales we are interested in, using the following sub-steps:\n",
    "\n",
    "1. Selecting residential buildings only\n",
    "2. Selecting only buildings with valid lat/lon coordinates\n",
    "3. Selecting sales occuring on or after Jan 1 2010\n",
    "4. Selecting only sales with non-null prices\n",
    "\n",
    "In the second step, we apply additional QA/QC filters and average cells up to our sampled grid cells, using the following steps:\n",
    "\n",
    "1. Dropping all sales under \\$10k\n",
    "2. Dropping all buildings under 100 sq ft\n",
    "3. Dropping all sales under \\$10/sq ft\n",
    "4. By state, dropping all sales above the 99th percentile of remaining sales in that state\n",
    "5. Keeping the latest sale if multiple sales of a given property remain.\n",
    "6. Taking the mean of remaining sales within each grid cell for the POP sample\n",
    "\n",
    "Performing this extraction, QA/QC, and aggregation in two separate steps allows one to test various QA/QC approaches without having to re-extract from the raw Zillow data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load grid, sample, and shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = dict(np.load(grid_path))\n",
    "sample = dict(np.load(sample_path))\n",
    "\n",
    "state_gdf = gpd.read_file(state_path)\n",
    "state_gdf[\"HASC_1\"] = state_gdf.HASC_1.str[-2:]\n",
    "state_gdf = state_gdf[~state_gdf.HASC_1.isin([\"PR\", \"HI\", \"AK\"])]\n",
    "\n",
    "## turn lats and lons from full grid into dataframe with lat/lon as index and i/j as value\n",
    "lons = (\n",
    "    pd.DataFrame(grid[\"lon\"], columns=[\"lon\"])\n",
    "    .reset_index()\n",
    "    .set_index(\"lon\")\n",
    "    .rename(columns={\"index\": \"j\"})\n",
    ")\n",
    "lats = (\n",
    "    pd.DataFrame(grid[\"lat\"], columns=[\"lat\"])\n",
    "    .reset_index()\n",
    "    .set_index(\"lat\")\n",
    "    .rename(columns={\"index\": \"i\"})\n",
    ")\n",
    "\n",
    "## adjust for 1-indexed i and j\n",
    "lons += 1\n",
    "lats += 1\n",
    "\n",
    "# need to flip lats b/c it's monotonic decreasing\n",
    "lats = lats.sort_index()\n",
    "\n",
    "# get list of sampled data points\n",
    "this_id = sample[\"ID\"].astype(str)\n",
    "sample_ix = pd.MultiIndex.from_tuples(\n",
    "    [tuple([int(j) for j in i.split(\",\")]) for i in this_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(func, it, print_progress=False):\n",
    "    result = []\n",
    "    for ix, i in enumerate(it):\n",
    "        if print_progress and (ix % 5 == 0):\n",
    "            print(\"Line {}\".format(ix))\n",
    "        this_res = func(i)\n",
    "        if this_res.shape[0] > 0:\n",
    "            result.append(this_res)\n",
    "    result = pd.concat(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def hash_rowid(df, index_cols=\"rowID\"):\n",
    "    dup_ids = df[\"rowID\"].duplicated(keep=False).sum()\n",
    "    df[\"rowID\"] = df[\"rowID\"].apply(lambda x: hash(x))\n",
    "\n",
    "    # check we havent created hash collisions\n",
    "    assert df[\"rowID\"].duplicated(keep=False).sum() == dup_ids\n",
    "\n",
    "    df = df.set_index(index_cols, drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sampled_val(this_st, this_geom, chunksize=1000000, settings=c_ext):\n",
    "\n",
    "    # improves performance to not check this\n",
    "    pd.options.mode.chained_assignment = None\n",
    "\n",
    "    ## GET RELEVANT FILES AND POINTS\n",
    "    # find points in this state\n",
    "    points_in_st = shp.vectorized.contains(this_geom, sample[\"lon\"], sample[\"lat\"])\n",
    "    this_id = sample[\"ID\"][points_in_st].astype(str)\n",
    "    this_st_fips = state_codes[this_st]\n",
    "    trans_trans_main_path = join(zillow_trans_path, this_st_fips, \"ZTrans\", \"Main.txt\")\n",
    "    trans_trans_prop_path = join(\n",
    "        zillow_trans_path, this_st_fips, \"ZTrans\", \"PropertyInfo.txt\"\n",
    "    )\n",
    "    trans_asmt_main_path = join(zillow_trans_path, this_st_fips, \"ZAsmt\", \"Main.txt\")\n",
    "    asmt_asmt_main_path = join(zillow_as_path, this_st_fips, \"ZAsmt\", \"Main.txt\")\n",
    "    trans_asmt_bldg_path = join(\n",
    "        zillow_trans_path, this_st_fips, \"ZAsmt\", \"Building.txt\"\n",
    "    )\n",
    "    asmt_asmt_bldg_path = join(zillow_as_path, this_st_fips, \"ZAsmt\", \"Building.txt\")\n",
    "    trans_asmt_bldgArea_path = join(\n",
    "        zillow_trans_path, this_st_fips, \"ZAsmt\", \"BuildingAreas.txt\"\n",
    "    )\n",
    "    asmt_asmt_bldgArea_path = join(\n",
    "        zillow_as_path, this_st_fips, \"ZAsmt\", \"BuildingAreas.txt\"\n",
    "    )\n",
    "    out_path = join(ztrax_dir_int, \"outcomes_all_{}_{}.pickle\".format(app, this_st))\n",
    "    out_path_agg = out_path.replace(\".pickle\", \"_aggregated.pickle\")\n",
    "\n",
    "    if isfile(out_path_agg):\n",
    "        return pd.read_pickle(out_path_agg)\n",
    "\n",
    "    ## ##################\n",
    "    ## TRANSACTION/ZTRANS\n",
    "    ## ##################\n",
    "    print(\"Loading trans/trans/main\", this_st)\n",
    "    ## Main\n",
    "    it = pd.read_csv(\n",
    "        trans_trans_main_path,\n",
    "        delimiter=\"|\",\n",
    "        usecols=[0, 6, 24, 25, 30],\n",
    "        names=[\"rowID\", \"date\", \"price\", \"sale_code\", \"if_xfer\"],\n",
    "        index_col=0,\n",
    "        chunksize=chunksize,\n",
    "        dtype={\n",
    "            \"rowID\": int,\n",
    "            \"price\": float,\n",
    "            \"sale_code\": str,\n",
    "            \"if_xfer\": str,\n",
    "            \"date\": str,\n",
    "        },\n",
    "        quoting=3,\n",
    "    )\n",
    "\n",
    "    def get_trans_trans_main(trans_info):\n",
    "        # drop missing dates, 0-priced sales, and intra-family transfers\n",
    "        trans_info = trans_info[\n",
    "            (trans_info[\"date\"] != \"\")\n",
    "            & (trans_info[\"date\"].notnull())\n",
    "            & (trans_info[\"price\"] > 0)\n",
    "            & (trans_info[\"if_xfer\"] != \"Y\")\n",
    "        ]\n",
    "        trans_info[\"date\"] = pd.to_datetime(\n",
    "            trans_info[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        # keep only sales after date\n",
    "        trans_info = trans_info[\n",
    "            trans_info.date\n",
    "            > pd.to_datetime(\"{}-01-01\".format(settings[\"cutoff_yr\"]), errors=\"coerce\")\n",
    "        ]\n",
    "\n",
    "        # drop if_xfer column\n",
    "        trans_info = trans_info.drop(columns=\"if_xfer\")\n",
    "        return trans_info\n",
    "\n",
    "    trans_trans_main = get_df(get_trans_trans_main, it)\n",
    "\n",
    "    print(\"Loading trans/trans/propertyInfo\", this_st)\n",
    "    ## Prop\n",
    "    it = pd.read_csv(\n",
    "        trans_trans_prop_path,\n",
    "        delimiter=\"|\",\n",
    "        usecols=[0, 52, 53, 64],\n",
    "        names=[\"rowID\", \"lat\", \"lon\", \"parcelID\"],\n",
    "        index_col=0,\n",
    "        dtype={\"rowID\": int, \"lat\": str, \"lon\": str, \"parcelID\": str},\n",
    "        chunksize=chunksize,\n",
    "        quoting=3,\n",
    "    )\n",
    "\n",
    "    def get_trans_trans_prop(prop_info):\n",
    "        prop_info[[\"lon\", \"lat\"]] = prop_info[[\"lon\", \"lat\"]].apply(\n",
    "            pd.to_numeric, errors=\"coerce\"\n",
    "        )\n",
    "        return prop_info.dropna(how=\"all\")\n",
    "\n",
    "    trans_trans_prop = get_df(get_trans_trans_prop, it)\n",
    "\n",
    "    ## Merge prop and main\n",
    "    merged = trans_trans_main.join(trans_trans_prop, how=\"left\")\n",
    "    del trans_trans_main, trans_trans_prop\n",
    "\n",
    "    ## Keep only if it has parcel ID in order\n",
    "    # to match to land use code\n",
    "    merged = merged.dropna(subset=[\"parcelID\"])\n",
    "\n",
    "    ## set index to parcelID b/c no longer need rowID\n",
    "    merged = merged.set_index(\"parcelID\", drop=True)\n",
    "\n",
    "    ## ##################\n",
    "    ## TRANSACTION/ZASMT\n",
    "    ## ##################\n",
    "\n",
    "    ## Main\n",
    "    print(\"Loading trans/asmt/main\", this_st)\n",
    "    it = pd.read_csv(\n",
    "        trans_asmt_main_path,\n",
    "        delimiter=\"|\",\n",
    "        usecols=[0, 1, 81, 82],\n",
    "        names=[\"rowID\", \"parcelID\", \"lat_asmt\", \"lon_asmt\"],\n",
    "        quoting=3,\n",
    "        dtype={\"rowID\": str, \"parcelID\": str, \"lat_asmt\": str, \"lon_asmt\": str},\n",
    "        chunksize=chunksize,\n",
    "    )\n",
    "\n",
    "    def get_asmt_main(asmt_info):\n",
    "        asmt_info = hash_rowid(asmt_info)\n",
    "        asmt_info[[\"lon_asmt\", \"lat_asmt\"]] = asmt_info[[\"lon_asmt\", \"lat_asmt\"]].apply(\n",
    "            pd.to_numeric, errors=\"coerce\"\n",
    "        )\n",
    "        return asmt_info\n",
    "\n",
    "    trans_asmt_main = get_df(get_asmt_main, it)\n",
    "\n",
    "    ## Building\n",
    "    print(\"Loading trans/asmt/building\", this_st)\n",
    "    it = pd.read_csv(\n",
    "        trans_asmt_bldg_path,\n",
    "        delimiter=\"|\",\n",
    "        usecols=[0, 5],\n",
    "        names=[\"rowID\", \"landuse\"],\n",
    "        quoting=3,\n",
    "        infer_datetime_format=True,\n",
    "        dtype={\"rowID\": str, \"landuse\": str},\n",
    "        chunksize=chunksize,\n",
    "    )\n",
    "\n",
    "    def get_asmt_bldg(bldg_info):\n",
    "        bldg_info = hash_rowid(bldg_info)\n",
    "        bldg_info = bldg_info.dropna()\n",
    "        bldg_info.landuse = bldg_info.landuse.apply(lambda x: x[:2])\n",
    "        bldg_info = bldg_info[\n",
    "            (bldg_info.landuse.isin(settings[\"use_codes_include\"]))\n",
    "            & ~(bldg_info.landuse.isin(settings[\"use_codes_not_include\"]))\n",
    "        ]\n",
    "        return bldg_info\n",
    "\n",
    "    trans_asmt_bldg = get_df(get_asmt_bldg, it)\n",
    "    res_bldgs = trans_asmt_bldg.index\n",
    "\n",
    "    ## Reindex main to only have residential buildings\n",
    "    trans_asmt_main = trans_asmt_main.reindex(res_bldgs).dropna(subset=[\"parcelID\"])\n",
    "\n",
    "    ## Building Area\n",
    "    print(\"Loading trans/asmt/buildingArea\", this_st)\n",
    "    it = pd.read_csv(\n",
    "        trans_asmt_bldgArea_path,\n",
    "        delimiter=\"|\",\n",
    "        usecols=[0, 1, 4],\n",
    "        names=[\"rowID\", \"imp\", \"sqft\"],\n",
    "        dtype={\"rowID\": str, \"imp\": int, \"sqft\": float},\n",
    "        chunksize=chunksize,\n",
    "        quoting=3,\n",
    "    )\n",
    "\n",
    "    def get_asmt_bldgArea(asmt_bldgArea):\n",
    "        asmt_bldgArea = hash_rowid(asmt_bldgArea, index_cols=[\"rowID\", \"imp\"])\n",
    "        # take whatever is the largest building area code as measure of sqfootage\n",
    "        asmt_bldgArea = asmt_bldgArea.groupby(level=[0, 1]).max()\n",
    "\n",
    "        # sum over multiple improvements on land\n",
    "        asmt_bldgArea = asmt_bldgArea.groupby(level=0).sum()\n",
    "\n",
    "        return asmt_bldgArea\n",
    "\n",
    "    trans_asmt_bldgArea = get_df(get_asmt_bldgArea, it)\n",
    "\n",
    "    # Merge sqft data into main\n",
    "    trans_asmt_main = trans_asmt_main.join(trans_asmt_bldgArea, how=\"left\")\n",
    "\n",
    "    ## Set index to parcelID\n",
    "    trans_asmt_main = trans_asmt_main.set_index(\"parcelID\", drop=True)\n",
    "\n",
    "    ## Inner merge with trans/trans to get properties with price data\n",
    "    # and residential classification\n",
    "    merged_trans = merged.join(trans_asmt_main, how=\"inner\")\n",
    "    del trans_asmt_main, trans_asmt_bldg, trans_asmt_bldgArea, res_bldgs\n",
    "\n",
    "    # fill in lat/lon from trans/asmt/main data\n",
    "    for l in [\"lat\", \"lon\"]:\n",
    "        merged_trans[l] = merged_trans[l].fillna(merged_trans[l + \"_asmt\"])\n",
    "        del merged_trans[l + \"_asmt\"]\n",
    "\n",
    "    ## ##################\n",
    "    ## ASSESSOR/ZASMT\n",
    "    ## ##################\n",
    "\n",
    "    ## Main\n",
    "    print(\"Loading asessor/asmt/main\", this_st)\n",
    "    it = pd.read_csv(\n",
    "        asmt_asmt_main_path,\n",
    "        delimiter=\"|\",\n",
    "        usecols=[0, 1, 6, 81, 82],\n",
    "        names=[\"rowID\", \"parcelID\", \"extract_date\", \"lat_asmt\", \"lon_asmt\"],\n",
    "        quoting=3,\n",
    "        chunksize=chunksize,\n",
    "        dtype={\n",
    "            \"rowID\": str,\n",
    "            \"parcelID\": str,\n",
    "            \"extract_date\": str,\n",
    "            \"lat_asmt\": str,\n",
    "            \"lon_asmt\": str,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def get_asmt_asmt_main(asmt2_info):\n",
    "        asmt2_info = hash_rowid(asmt2_info)\n",
    "        asmt2_info[[\"lat_asmt\", \"lon_asmt\"]] = asmt2_info[\n",
    "            [\"lat_asmt\", \"lon_asmt\"]\n",
    "        ].apply(pd.to_numeric, errors=\"coerce\")\n",
    "        asmt2_info[\"extract_date\"] = pd.to_datetime(\n",
    "            asmt2_info[\"extract_date\"], format=\"%m%Y\", errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        # drop if rowID, parcelID or lat/lon missing\n",
    "        asmt2_info = asmt2_info[\n",
    "            (asmt2_info.index.notnull())\n",
    "            & (asmt2_info[[\"lat_asmt\", \"lon_asmt\", \"parcelID\"]].notnull().all(axis=1))\n",
    "        ]\n",
    "        return asmt2_info\n",
    "\n",
    "    asmt_asmt_main = get_df(get_asmt_asmt_main, it, print_progress=True)\n",
    "    print(\"Completed asessor/asmt/main\", this_st)\n",
    "\n",
    "    ## Building\n",
    "    print(\"Loading asessor/asmt/building\", this_st)\n",
    "    it = pd.read_csv(\n",
    "        asmt_asmt_bldg_path,\n",
    "        delimiter=\"|\",\n",
    "        usecols=[0, 5],\n",
    "        names=[\"rowID\", \"landuse\"],\n",
    "        quoting=3,\n",
    "        infer_datetime_format=True,\n",
    "        dtype={\"rowID\": str, \"landuse\": str},\n",
    "        chunksize=chunksize,\n",
    "    )\n",
    "    asmt_asmt_bldg = get_df(get_asmt_bldg, it)\n",
    "    res_bldgs_asmt = asmt_asmt_bldg.index\n",
    "\n",
    "    # Reindex main to only have residential buildings\n",
    "    asmt_asmt_main = asmt_asmt_main.reindex(res_bldgs_asmt).dropna()\n",
    "\n",
    "    # Take only the latest extraction date by parcel ID\n",
    "    # (theoretically most up-to-date location)\n",
    "    # IB 1/23/19: More thorough approach would be to match to sales\n",
    "    # first and THEN take extraction date closest to sale date\n",
    "    asmt_asmt_main = (\n",
    "        asmt_asmt_main.sort_values(\"extract_date\")\n",
    "        .reset_index(drop=False)\n",
    "        .groupby(\"parcelID\")\n",
    "        .last()\n",
    "        .reset_index(drop=False)\n",
    "        .set_index(\"rowID\", drop=True)\n",
    "    )\n",
    "\n",
    "    ## Building Area\n",
    "    print(\"Loading asessor/asmt/buildingArea\", this_st)\n",
    "    it = pd.read_csv(\n",
    "        asmt_asmt_bldgArea_path,\n",
    "        delimiter=\"|\",\n",
    "        usecols=[0, 1, 4],\n",
    "        names=[\"rowID\", \"imp\", \"sqft\"],\n",
    "        dtype={\"rowID\": str, \"imp\": int, \"sqft\": float},\n",
    "        chunksize=chunksize,\n",
    "        quoting=3,\n",
    "    )\n",
    "    asmt_asmt_bldgArea = get_df(get_asmt_bldgArea, it)\n",
    "\n",
    "    # Merge sqft data into main\n",
    "    asmt_asmt_main = asmt_asmt_main.join(asmt_asmt_bldgArea, how=\"left\").set_index(\n",
    "        \"parcelID\", drop=True\n",
    "    )\n",
    "\n",
    "    ## Inner merge with trans/trans to get properties with price data\n",
    "    # and residential classification\n",
    "    merged_asmt = merged.join(asmt_asmt_main, how=\"inner\")\n",
    "    del asmt_asmt_main, asmt_asmt_bldg, res_bldgs_asmt, asmt_asmt_bldgArea\n",
    "\n",
    "    # fill in lat/lon from trans/asmt/main data\n",
    "    for l in [\"lat\", \"lon\"]:\n",
    "        merged_asmt[l] = merged_asmt[l].fillna(merged_asmt[l + \"_asmt\"])\n",
    "        del merged_asmt[l + \"_asmt\"]\n",
    "\n",
    "    ## ##################\n",
    "    ## COMBINE ALL FILES\n",
    "    ## ##################\n",
    "    merged_all = merged_trans.join(merged_asmt, how=\"outer\", rsuffix=\"_asmt\")\n",
    "    del merged_trans, merged_asmt\n",
    "\n",
    "    # merge data from asmt/asmt and trans/asmt\n",
    "    cols = [\"date\", \"price\", \"sale_code\", \"lat\", \"lon\", \"sqft\"]\n",
    "    cols_asmt = [c + \"_asmt\" for c in cols]\n",
    "    change_dict = dict([(cols_asmt[cx], c) for cx, c in enumerate(cols)])\n",
    "    merged_all = merged_all.loc[:, cols].fillna(\n",
    "        merged_all.loc[:, cols_asmt].rename(columns=change_dict)\n",
    "    )\n",
    "\n",
    "    # drop any missing that remained\n",
    "    merged_all = merged_all.dropna(subset=[\"price\", \"lat\", \"lon\"])\n",
    "\n",
    "    # get column for price only if sqft is not null\n",
    "    merged_all[\"price_hasSize\"] = merged_all[\"price\"].where(\n",
    "        merged_all[\"sqft\"].notnull()\n",
    "    )\n",
    "\n",
    "    # add state column and drop unneeded sale code\n",
    "    merged_all[\"state\"] = this_st\n",
    "    merged_all = merged_all.drop(columns=[\"sale_code\"])\n",
    "\n",
    "    ## aggregating ##\n",
    "    merged_all = merged_all.sort_values(by=\"lon\")\n",
    "    merged_all[\"j\"] = pd.merge_asof(\n",
    "        merged_all[[\"lon\"]], lons, left_on=\"lon\", direction=\"nearest\", right_index=True\n",
    "    ).loc[:, \"j\"]\n",
    "\n",
    "    merged_all = merged_all.sort_values(by=\"lat\")\n",
    "    merged_all[\"i\"] = pd.merge_asof(\n",
    "        merged_all[[\"lat\"]], lats, left_on=\"lat\", direction=\"nearest\", right_index=True\n",
    "    ).loc[:, \"i\"]\n",
    "\n",
    "    # save merged but not aggregated/sampled data\n",
    "    merged_all.to_pickle(out_path)\n",
    "\n",
    "    ## ##################\n",
    "    ## QA/QC + Aggregation\n",
    "    ## ##################\n",
    "\n",
    "    price_cols = [\"price\", \"price_per_sqft\"]\n",
    "\n",
    "    ### QA/QC\n",
    "    # Drop if <= $10k\n",
    "    merged_all = merged_all[merged_all[\"price\"] > 10000]\n",
    "\n",
    "    # drop price/sqft if sqft <= 100\n",
    "    merged_all[\"sqft\"] = merged_all[\"sqft\"].where(merged_all[\"sqft\"] > 100)\n",
    "    merged_all[\"price_per_sqft\"] = merged_all[\"price_hasSize\"] / merged_all[\"sqft\"]\n",
    "\n",
    "    # drop price/sqft if price/sqft <= $10\n",
    "    merged_all[\"price_per_sqft\"] = merged_all[\"price_per_sqft\"].where(\n",
    "        merged_all[\"price_per_sqft\"] > 10\n",
    "    )\n",
    "\n",
    "    # Drop obs over Kth percentile by state\n",
    "    tile = cfg.housing[\"data\"][\"ztrax\"][\"pctile_clip\"]\n",
    "    maxs = merged_all[price_cols].quantile(tile)\n",
    "    merged_all.loc[:, price_cols] = merged_all.loc[:, price_cols].where(\n",
    "        merged_all.loc[:, price_cols] <= maxs\n",
    "    )\n",
    "    merged_all = merged_all.dropna(subset=price_cols, how=\"all\")\n",
    "\n",
    "    # Keep only latest sale\n",
    "    merged_all = merged_all.sort_values(by=\"date\")\n",
    "    merged_all = merged_all[~merged_all.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    ### Aggregate over images\n",
    "    # Keep just the columns you need\n",
    "    merged_all = merged_all.drop(columns=[\"date\", \"sqft\", \"price_hasSize\", \"state\"])\n",
    "\n",
    "    ## aggregating ##\n",
    "    grouped = merged_all[price_cols + [\"i\", \"j\"]].groupby([\"i\", \"j\"])\n",
    "\n",
    "    # collapse to grid cell using mean and geometric mean\n",
    "    agg_val = grouped.mean()\n",
    "    agg_val.columns = price_cols\n",
    "    agg_val_geom = grouped.agg(lambda x: gmean(x.dropna()))\n",
    "    agg_val_geom.columns = [p + \"_geomMean\" for p in price_cols]\n",
    "    agg_count = grouped.count()\n",
    "    count_cols = [\"n_obs_\" + p for p in price_cols]\n",
    "    agg_count.columns = count_cols\n",
    "    out_all = pd.concat([agg_val, agg_val_geom, agg_count], axis=1)\n",
    "\n",
    "    # take just sample cells (sample_ix has sampled id's)\n",
    "    out_df = out_all.reindex(index=sample_ix)\n",
    "    out_df = out_df.dropna(subset=price_cols, how=\"all\")\n",
    "\n",
    "    # If the number of obs col is missing, there are no houses in that image tile\n",
    "    out_df.loc[:, count_cols] = out_df.loc[:, count_cols].fillna(0)\n",
    "\n",
    "    # combine multiindex\n",
    "    out_df.index = (\n",
    "        out_df.index.get_level_values(0).astype(str)\n",
    "        + \",\"\n",
    "        + out_df.index.get_level_values(1).astype(str)\n",
    "    )\n",
    "    out_df.index.name = \"ID\"\n",
    "\n",
    "    for col in count_cols:\n",
    "        out_df[col] = out_df[col].astype(int)\n",
    "\n",
    "    # save version with just clipped values\n",
    "    out_df.to_pickle(out_path.replace(\".pickle\", \"_aggregated.pickle\"))\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bab780b9064e0b9e1c180a8598c220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ftrs = client.map(\n",
    "    get_sampled_val,\n",
    "    state_gdf.HASC_1.values,\n",
    "    state_gdf.geometry.values,\n",
    "    chunksize=chunksize,\n",
    ")\n",
    "progress(ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat(client.gather(ftrs))\n",
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with grid cells that overlapped states\n",
    "data_df.loc[:, [\"price\", \"price_per_sqft\"]] = (\n",
    "    data_df.loc[:, [\"price\", \"price_per_sqft\"]]\n",
    "    * data_df.loc[:, [\"n_obs_price\", \"n_obs_price_per_sqft\"]].values\n",
    ")\n",
    "data_df.loc[:, [\"price_geomMean\", \"price_per_sqft_geomMean\"]] = (\n",
    "    np.log(data_df.loc[:, [\"price_geomMean\", \"price_per_sqft_geomMean\"]])\n",
    "    * data_df.loc[:, [\"n_obs_price\", \"n_obs_price_per_sqft\"]].values\n",
    ")\n",
    "data_df = data_df.groupby(level=0).sum()\n",
    "data_df.loc[:, [\"price\", \"price_per_sqft\"]] = (\n",
    "    data_df.loc[:, [\"price\", \"price_per_sqft\"]]\n",
    "    / data_df.loc[:, [\"n_obs_price\", \"n_obs_price_per_sqft\"]].values\n",
    ")\n",
    "data_df.loc[:, [\"price_geomMean\", \"price_per_sqft_geomMean\"]] = np.exp(\n",
    "    data_df.loc[:, [\"price_geomMean\", \"price_per_sqft_geomMean\"]]\n",
    "    / data_df.loc[:, [\"n_obs_price\", \"n_obs_price_per_sqft\"]].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "data_df.to_csv(cfg.outcomes_fpath, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86243 0.999315886506731 0.9324814767575339\n"
     ]
    }
   ],
   "source": [
    "# Give a sense of coverage\n",
    "total = data_df.shape[0]\n",
    "coverage_price = data_df.count()[\"price\"] / total\n",
    "coverage_sqft = data_df.count()[\"price_per_sqft\"] / total\n",
    "\n",
    "print(total, coverage_price, coverage_sqft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5e2efbc9b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df.n_obs_price_per_sqft.hist(bins=range(0, 1000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook makes and evaluates predictions on the holdout test set. It trains and evaluates a model using features from a pre-trained ResNet-152 (shown as one of the sets of bars in Fig. 3A) and saves them to root/results/figures/Fig3/TestSetR2_resnet152_1e5_pretrained_.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the settings you can adjust when running this notebook:\n",
    "- ``num_threads``: If running on a multi-core machine, change this from ``None`` to an ``int`` in order to set the max number of threads to use\n",
    "- ``subset_[n,feat]``: If you want to subset the training set data for quick tests/debugging, specify that here using the `slice` object. `slice(None)` implies no subsetting of the ~80k observations for each label that are in the training set. `subset_n` slices observations; `subset_feat` subsets features.\n",
    "- ``overwrite``: By default, this code will raise an error if the file you are saving already exists. If you would like to disable that and overwrite existing data files, change `overwrite` to `True`.\n",
    "- ``fixed_lambda``: If True, only run the lambda that was previously chosen. Will throw an error if you haven't already generated a results file.\n",
    "- ``labels_to_run``: By default, this notebook will loop through all the labels. If you would like, you can reduce this list to only loop through a subset of them, by changing ``\"all\"`` to a list of task names, e.g. ``[\"housing\", \"treecover\"]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "num_threads = None\n",
    "\n",
    "subset_n = slice(None)\n",
    "subset_feat = slice(None)\n",
    "\n",
    "overwrite = True\n",
    "\n",
    "fixed_lambda = False\n",
    "\n",
    "labels_to_run = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import io as b_io\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import dill\n",
    "\n",
    "# Import necessary packages\n",
    "from mosaiks import transforms\n",
    "from mosaiks.utils import OVERWRITE_EXCEPTION\n",
    "from mosaiks.utils.imports import *\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "if num_threads is not None:\n",
    "    threadpool_limits(num_threads)\n",
    "    os.environ[\"NUMBA_NUM_THREADS\"] = str(num_threads)\n",
    "\n",
    "if overwrite is None:\n",
    "    overwrite = os.getenv(\"MOSAIKS_OVERWRITE\", False)\n",
    "if labels_to_run == \"all\":\n",
    "    labels_to_run = c.app_order\n",
    "\n",
    "solver = solve.ridge_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Random Feature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads our features from a pre-trained ResNet-152 for both POP and UAR samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {}\n",
    "latlons = {}\n",
    "\n",
    "for sample in [\"UAR\", \"POP\"]:\n",
    "    # path to features X for UAR\n",
    "    features_path = (\n",
    "        Path(c.features_dir)\n",
    "        / f\"{c.features['pretrained']['model_type']}_pretrained_{c.grid['area']}_{c.images['zoom_level']}_{c.images['n_pixels']}_{sample}.pkl\"\n",
    "    )\n",
    "    with open(features_path, \"rb\") as f:\n",
    "        data = dill.load(f)\n",
    "    features = data[\"X\"].astype(np.float64)\n",
    "    latlons_samp = data[\"latlon\"]\n",
    "    ids_x = data[\"ids_X\"]\n",
    "\n",
    "    X[sample] = pd.DataFrame(features, index=ids_x)\n",
    "    latlons[sample] = pd.DataFrame(latlons_samp, index=ids_x, columns=[\"lat\", \"lon\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop will:\n",
    "1. Load the appropriate labels\n",
    "2. Merge them with the feature matrix\n",
    "3. Remove test set observations\n",
    "4. Run a ridge regression on the training/validation set, sweeping over a range of possible regularization parameters using 5-fold Cross-Validation and clipping predictions to pre-specified bounds.\n",
    "5. Using the optimal hyperparameters found from step 4, we retrain the model on the entire training set and then make a prediction in the test set. \n",
    "\n",
    "**Note**: We drop observations for which our labels are missing. In one label (nightlights) we drop an outlier that was not dropped during dataset pre-processing. For all variables that we model in log-space, we first add 1 to deal with 0-valued observations (except for housing, for which we do not have any 0's)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Running regressions for: treecover\n",
      "Loading labels...\n",
      "Merging labels and features...\n",
      "Splitting training/test...\n",
      "Training model...\n",
      "on fold (of 5): 1 2 3 4 5 \n",
      "*** Running regressions for: elevation\n",
      "Loading labels...\n",
      "Merging labels and features...\n",
      "Splitting training/test...\n",
      "Training model...\n",
      "on fold (of 5): 1 2 3 4 5 \n",
      "*** Running regressions for: population\n",
      "Loading labels...\n",
      "Merging labels and features...\n",
      "Splitting training/test...\n",
      "Training model...\n",
      "on fold (of 5): 1 2 3 4 5 \n",
      "*** Running regressions for: nightlights\n",
      "Loading labels...\n",
      "Merging labels and features...\n",
      "Splitting training/test...\n",
      "Training model...\n",
      "on fold (of 5): 1 2 3 4 5 \n",
      "*** Running regressions for: income\n",
      "Loading labels...\n",
      "Merging labels and features...\n",
      "Splitting training/test...\n",
      "Training model...\n",
      "on fold (of 5): 1 2 3 4 5 \n",
      "*** Running regressions for: roads\n",
      "Loading labels...\n",
      "Merging labels and features...\n",
      "Splitting training/test...\n",
      "Training model...\n",
      "on fold (of 5): 1 2 3 4 5 \n",
      "*** Running regressions for: housing\n",
      "Loading labels...\n",
      "Merging labels and features...\n",
      "Splitting training/test...\n",
      "Training model...\n",
      "on fold (of 5): 1 2 3 4 5 \n"
     ]
    }
   ],
   "source": [
    "resultsDict = {}\n",
    "for label in labels_to_run:\n",
    "\n",
    "    print(\"*** Running regressions for: {}\".format(label))\n",
    "\n",
    "    ## Set some label-specific variables\n",
    "    this_cfg = io.get_filepaths(c, label)\n",
    "    c_app = getattr(this_cfg, label)\n",
    "    sampling_type = c_app[\"sampling\"]  # UAR or POP\n",
    "\n",
    "    best_lambda_fpath = join(c.fig_dir_sec, \"best_lambda_tl.npz\")\n",
    "    if fixed_lambda:\n",
    "        lambdas = io.get_lambdas(c, label, best_lambda_fpath=best_lambda_fpath)\n",
    "    else:\n",
    "        lambdas = io.get_lambdas(c, label, best_lambda_fpath=None)\n",
    "\n",
    "    if c_app[\"logged\"]:\n",
    "        bounds = np.array([c_app[\"us_bounds_log_pred\"]])\n",
    "    else:\n",
    "        bounds = np.array([c_app[\"us_bounds_pred\"]])\n",
    "\n",
    "    # Set solver arguments\n",
    "    solver_kwargs = {\n",
    "        \"lambdas\": lambdas,\n",
    "        \"return_preds\": True,\n",
    "        \"svd_solve\": False,\n",
    "        \"clip_bounds\": bounds,\n",
    "    }\n",
    "\n",
    "    # Expand possible lambdas for this transfer learning feature set as needed so that\n",
    "    # the optimal lambda selected is not hitting the bounds of possible lambdas.\n",
    "    if (not fixed_lambda) and (label in [\"income\", \"roads\"]):\n",
    "        solver_kwargs[\"lambdas\"] = np.logspace(-3, 4, 9)\n",
    "\n",
    "    ## get X, Y, latlon values of training data\n",
    "    (\n",
    "        this_X,\n",
    "        this_X_test,\n",
    "        this_Y,\n",
    "        this_Y_test,\n",
    "        _,\n",
    "        _,\n",
    "    ) = parse.merge_dropna_transform_split_train_test(\n",
    "        this_cfg, label, X[sampling_type], latlons[sampling_type]\n",
    "    )\n",
    "\n",
    "    ## subset\n",
    "    this_X = this_X[subset_n, subset_feat]\n",
    "    this_X_test = this_X_test[:, subset_feat]\n",
    "    this_Y = this_Y[subset_n]\n",
    "\n",
    "    ## Train model using ridge regression and 5-fold cross-valiation\n",
    "    ## (number of folds can be adjusted using the argument n_folds)\n",
    "    print(\"Training model...\")\n",
    "    kfold_results = solve.kfold_solve(\n",
    "        this_X,\n",
    "        this_Y,\n",
    "        solve_function=solver,\n",
    "        num_folds=this_cfg.ml_model[\"n_folds\"],\n",
    "        return_model=True,\n",
    "        **solver_kwargs\n",
    "    )\n",
    "    print(\"\")\n",
    "\n",
    "    ## Store the metrics and the predictions from the best performing model\n",
    "    best_lambda_idx, best_metrics, best_preds = ir.interpret_kfold_results(\n",
    "        kfold_results, \"r2_score\", hps=[(\"lambdas\", solver_kwargs[\"lambdas\"])]\n",
    "    )\n",
    "    best_lambdas = np.array(\n",
    "        [solver_kwargs[\"lambdas\"][np.asarray(best_lambda_idx).squeeze()]]\n",
    "    )\n",
    "\n",
    "    # save best lambdas\n",
    "    if subset_n == slice(None) and subset_feat == slice(None):\n",
    "        np.savez(best_lambda_fpath, best_lambda=best_lambdas)\n",
    "\n",
    "    ## Retrain a model using this best lambda:\n",
    "    holdout_results = solve.single_solve(\n",
    "        this_X,\n",
    "        this_X_test,\n",
    "        this_Y,\n",
    "        this_Y_test,\n",
    "        lambdas=best_lambdas,\n",
    "        return_preds=True,\n",
    "        return_model=False,\n",
    "        clip_bounds=bounds,\n",
    "        svd_solve=False,\n",
    "    )\n",
    "\n",
    "    ## Store the R2\n",
    "    resultsDict[label] = holdout_results[\"metrics_test\"][0][0][0][\"r2_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print and save a table of the test set R2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get save path\n",
    "if (subset_n != slice(None)) or (subset_feat != slice(None)):\n",
    "    subset_str = \"_subset\"\n",
    "else:\n",
    "    subset_str = \"\"\n",
    "\n",
    "fn = Path(\n",
    "    c.data_dir\n",
    "    + \"/output/cnn_comparison/TestSetR2_resnet152_1e5_pretrained\"\n",
    "    + subset_str\n",
    "    + \".csv\"\n",
    ")\n",
    "\n",
    "if (not overwrite) and subset_str == \"\" and fn.is_file():\n",
    "    raise OVERWRITE_EXCEPTION\n",
    "\n",
    "# save\n",
    "with open(fn, \"w\") as f:\n",
    "    for key in resultsDict.keys():\n",
    "        f.write(\"%s,%s\\n\" % (key, resultsDict[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mosaiks-env]",
   "language": "python",
   "name": "conda-env-mosaiks-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

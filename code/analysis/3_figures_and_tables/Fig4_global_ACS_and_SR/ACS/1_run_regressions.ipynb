{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short notebook runs ridge regressions on the pre-featurized data matrix using a 5-fold cross-validation approach for all 12 ACS labels. It saves `.data` files that are then opened by the notebook \"2_make_fig.ipynb\" in this same folder to make the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the settings you can adjust when running this notebook:\n",
    "- ``num_threads``: If running on a multi-core machine, change this from ``None`` to an ``int`` in order to set the max number of threads to use\n",
    "- ``subset_[n,feat]``: If you want to subset the training set data for quick tests/debugging, specify that here using the `slice` object. `slice(None)` implies no subsetting of the ~80k observations for each label that are in the training set. `subset_n` slices observations; `subset_feat` subsets features.\n",
    "- ``overwrite``: By default, this code will raise an error if the file you are saving already exists. If you would like to disable that and overwrite existing data files, change `overwrite` to `True`.\n",
    "- ``labels_to_run``: By default, this notebook will loop through all the ACS labels. If you would like, you can reduce this list to only loop through a subset of them.\n",
    "- ``intercept``: Do you want to add an intercept to (a.k.a. center) the data? We used this for testing, confirmed that with a large enough dataset and high dimensionality, the intercept does not help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "num_threads = None\n",
    "\n",
    "subset_n = slice(None)\n",
    "subset_feat = slice(None)\n",
    "\n",
    "overwrite = None\n",
    "\n",
    "labels_to_run = [\n",
    "    \"B08303\",\n",
    "    \"B15003\",\n",
    "    \"B19013\",\n",
    "    \"B19301\",\n",
    "    \"C17002\",\n",
    "    \"B22010\",\n",
    "    \"B25071\",\n",
    "    \"B25001\",\n",
    "    \"B25002\",\n",
    "    \"B25035\",\n",
    "    \"B25017\",\n",
    "    \"B25077\",\n",
    "]\n",
    "\n",
    "intercept = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from os.path import isfile\n",
    "\n",
    "# Import necessary packages\n",
    "from mosaiks import transforms\n",
    "from mosaiks.utils.imports import *\n",
    "from mosaiks.utils import OVERWRITE_EXCEPTION\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "if num_threads is not None:\n",
    "    threadpool_limits(num_threads)\n",
    "    os.environ[\"NUMBA_NUM_THREADS\"] = str(num_threads)\n",
    "\n",
    "if overwrite is None:\n",
    "    overwrite = os.getenv(\"MOSAIKS_OVERWRITE\", False)\n",
    "if labels_to_run == \"all\":\n",
    "    labels_to_run = c.app_order\n",
    "\n",
    "solver = solve.ridge_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pattern will be used as the filename to save the out-of-sample predictions for each label. If you wish to try your own analysis and not overwrite the data as it exists, **you must change the name**. Regardless, the `2_make_fig.ipynb` notebook will look for model prediction `.data` files matching the default pattern below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_patt = (\n",
    "    \"outcomes_scatter_obsAndPred_{{app}}_{{variable}}_CONTUS_16_640_{{sampling_type}}_{sampling_num}_\"\n",
    "    \"{sampling_seed}_random_features_{patch_size}_{feature_seed}{{subset}}.data\"\n",
    ").format(\n",
    "    sampling_num=c.sampling[\"n_samples\"],\n",
    "    sampling_seed=c.sampling[\"seed\"],\n",
    "    patch_size=c.features[\"random\"][\"patch_size\"],\n",
    "    feature_seed=c.features[\"random\"][\"seed\"],\n",
    "    intercept=intercept,\n",
    ")\n",
    "save_patt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load X Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads our feature matrix `X` for both POP and UAR samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {}\n",
    "latlons = {}\n",
    "\n",
    "X[\"UAR\"], latlons[\"UAR\"] = io.get_X_latlon(c, \"UAR\")\n",
    "X[\"POP\"], latlons[\"POP\"] = io.get_X_latlon(c, \"POP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop will:\n",
    "1. Load the appropriate labels\n",
    "2. Merge them with the feature matrix\n",
    "3. Remove test set observations\n",
    "4. Run a ridge regression on the training/validation set, sweeping over a range of possible regularization parameters using 5-fold Cross-Validation and clipping predictions to pre-specified bounds.\n",
    "5. Save the out-of-sample predictions and the observations for use in the ACS figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run each label separately b/c of different missingness patterns across labels\n",
    "for label in labels_to_run:\n",
    "    print(label)\n",
    "    print(\"*** Running regressions for: {}\".format(label))\n",
    "\n",
    "    ## Set some label-specific variables\n",
    "    this_cfg = io.get_filepaths(c, label, \"random\", True)\n",
    "    c_app = getattr(this_cfg, label)\n",
    "    sampling_type = c_app[\"sampling\"]  # UAR or POP\n",
    "    print(sampling_type)\n",
    "\n",
    "    ## Get save path\n",
    "    if (subset_n != slice(None)) or (subset_feat != slice(None)):\n",
    "        subset_str = \"_subset\"\n",
    "    else:\n",
    "        subset_str = \"\"\n",
    "    save_path = join(\n",
    "        c.fig_dir,\n",
    "        \"primary_analysis\",\n",
    "        save_patt.format(\n",
    "            app=label,\n",
    "            variable=c_app[\"variable\"],\n",
    "            sampling_type=sampling_type,\n",
    "            subset=subset_str,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if (not overwrite) and isfile(save_path):\n",
    "        raise OVERWRITE_EXCEPTION\n",
    "\n",
    "    ## get X, Y, latlon values of training data\n",
    "    (\n",
    "        this_X,\n",
    "        _,\n",
    "        this_Y,\n",
    "        _,\n",
    "        this_latlons,\n",
    "        _,\n",
    "    ) = parse.merge_dropna_transform_split_train_test(\n",
    "        this_cfg, label, X[sampling_type], latlons[sampling_type], ACS=True\n",
    "    )\n",
    "\n",
    "    ## subset\n",
    "    this_latlons = this_latlons[subset_n]\n",
    "    this_X = this_X[subset_n, subset_feat]\n",
    "    this_Y = this_Y[subset_n]\n",
    "\n",
    "    ##Set solver arguments\n",
    "    bounds_this = [np.array(c_app[\"us_bounds_pred\"])]\n",
    "    print(\"lambdas are \", c_app[\"lambdas\"])\n",
    "    solver_kwargs = {\n",
    "        \"lambdas\": c_app[\"lambdas\"],\n",
    "        \"return_preds\": True,\n",
    "        \"svd_solve\": False,\n",
    "        \"clip_bounds\": bounds_this,\n",
    "        \"intercept\": intercept,\n",
    "    }\n",
    "\n",
    "    ## Train model using ridge regression and 5-fold cross-valiation\n",
    "    ## (number of folds can be adjusted using the argument n_folds)\n",
    "    print(\"Training model...\")\n",
    "    kfold_results = solve.kfold_solve(\n",
    "        this_X,\n",
    "        this_Y,\n",
    "        solve_function=solver,\n",
    "        num_folds=this_cfg.ml_model[\"n_folds\"],\n",
    "        return_model=True,\n",
    "        **solver_kwargs\n",
    "    )\n",
    "    print(\"\")\n",
    "\n",
    "    ## Store the metrics and the predictions from the best performing model\n",
    "    best_lambda_idx, best_metrics, best_preds = ir.interpret_kfold_results(\n",
    "        kfold_results, \"r2_score\", hps=[(\"lambdas\", solver_kwargs[\"lambdas\"])]\n",
    "    )\n",
    "\n",
    "    ## combine out-of-sample predictions over folds\n",
    "    preds = np.vstack([solve.y_to_matrix(i) for i in best_preds.squeeze()]).squeeze()\n",
    "    truth = np.vstack(\n",
    "        [solve.y_to_matrix(i) for i in kfold_results[\"y_true_test\"].squeeze()]\n",
    "    ).squeeze()\n",
    "\n",
    "    # get latlons in same shuffled, cross-validated order\n",
    "    ll = this_latlons[\n",
    "        np.hstack([test for train, test in kfold_results[\"cv\"].split(this_latlons)])\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"truth\": truth,\n",
    "        \"preds\": preds,\n",
    "        \"lon\": ll[:, 1],\n",
    "        \"lat\": ll[:, 0],\n",
    "        \"bounds\": bounds_this,\n",
    "        \"best_lambda_idx\": best_lambda_idx[0][0],\n",
    "    }\n",
    "\n",
    "    print(\"Saving model to {}\".format(save_path))\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell checks whether we hit the bounds of our search space for the regularization parameter. For each label, it prints the index of the chosen parameter value, as well as the index of the highest value. If the chosen value index is either 0 or equal to that of the highest value, you should extend your search space using the `config.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels_to_run:\n",
    "    print(label)\n",
    "\n",
    "    ## Set some label-specific variables\n",
    "    this_cfg = io.get_filepaths(c, label, \"random\", True)\n",
    "    c_app = getattr(this_cfg, label)\n",
    "    sampling_type = c_app[\"sampling\"]  # UAR or POP\n",
    "\n",
    "    ## Get save path\n",
    "    if (subset_n != slice(None)) or (subset_feat != slice(None)):\n",
    "        subset_str = \"_subset\"\n",
    "    else:\n",
    "        subset_str = \"\"\n",
    "    save_path = join(\n",
    "        c.fig_dir,\n",
    "        \"primary_analysis\",\n",
    "        save_patt.format(\n",
    "            app=label,\n",
    "            variable=c_app[\"variable\"],\n",
    "            sampling_type=sampling_type,\n",
    "            subset=subset_str,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    with open(save_path, \"rb\") as f:\n",
    "        x = pickle.load(f)\n",
    "        try:\n",
    "            print(x[\"best_lambda_idx\"], len(c_app[\"lambdas\"]) - 1)\n",
    "        except KeyError:\n",
    "            print(\"not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

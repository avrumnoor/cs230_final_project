{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection: Logs and levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook makes and evaluates predictions for each task under two models. The first logs the outcome variable, while the second keeps the outcome variable in levels.\n",
    "The notebook creates the results shown in Supplementary Table S3 and saves them to root/results/tables/TableS3/LogsVsLevels.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the settings you can adjust when running this notebook:\n",
    "- ``num_threads``: If running on a multi-core machine, change this from ``None`` to an ``int`` in order to set the max number of threads to use\n",
    "- ``subset_[n,feat]``: If you want to subset the training set data for quick tests/debugging, specify that here using the `slice` object. `slice(None)` implies no subsetting of the ~80k observations for each label that are in the training set. `subset_n` slices observations; `subset_feat` subsets features.\n",
    "- ``overwrite``: By default, this code will raise an error if the file you are saving already exists. If you would like to disable that and overwrite existing data files, change `overwrite` to `True`.\n",
    "- ``labels_to_run``: By default, this notebook will loop through all the labels. If you would like, you can reduce this list to only loop through a subset of them, by changing ``\"all\"`` to a list of task names, e.g. ``[\"housing\", \"treecover\"]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "num_threads = None\n",
    "\n",
    "subset_n = slice(None)\n",
    "subset_feat = slice(None)\n",
    "\n",
    "overwrite = None\n",
    "\n",
    "labels_to_run = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from mosaiks import transforms\n",
    "from mosaiks.utils import OVERWRITE_EXCEPTION\n",
    "from mosaiks.utils.imports import *\n",
    "from sklearn import metrics\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "if num_threads is not None:\n",
    "    threadpool_limits(num_threads)\n",
    "    os.environ[\"NUMBA_NUM_THREADS\"] = str(num_threads)\n",
    "\n",
    "if overwrite is None:\n",
    "    overwrite = os.getenv(\"MOSAIKS_OVERWRITE\", False)\n",
    "if labels_to_run == \"all\":\n",
    "    labels_to_run = c.app_order\n",
    "\n",
    "solver = solve.ridge_regression\n",
    "\n",
    "out_dir = Path(c.res_dir, \"tables\", \"TableS3\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Random Feature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads our feature matrix `X` for both POP and UAR samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {}\n",
    "latlons = {}\n",
    "\n",
    "X[\"UAR\"], latlons[\"UAR\"] = io.get_X_latlon(c, \"UAR\")\n",
    "X[\"POP\"], latlons[\"POP\"] = io.get_X_latlon(c, \"POP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop will:\n",
    "1. Load the appropriate labels\n",
    "2. Merge them with the feature matrix\n",
    "3. Remove test set observations\n",
    "4. Run two sets of ridge regressions on the training/validation set, sweeping over a range of possible regularization parameters using 5-fold Cross-Validation and clipping predictions to pre-specified bounds. The first set uses the outcome variable in levels, the second set uses the outcome variable in logs.\n",
    "5. Store two R2 values per label, one from the levels model, one from the log model.  \n",
    "\n",
    "**Note**: We drop observations for which our labels are missing. In one label (nightlights) we drop an outlier that was not dropped during dataset pre-processing. In another (elevation), we convert a small number of negative values to zero in order to take log transformations. For all variables that we model in log-space, we first add 1 to deal with 0-valued observations (except for housing, for which we do not have any 0's)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDict = {}\n",
    "for label in labels_to_run:\n",
    "\n",
    "    print(\"*** Running regressions for: {}\".format(label))\n",
    "\n",
    "    ## Set some label-specific variables\n",
    "    this_cfg = io.get_filepaths(c, label)\n",
    "    c_app = getattr(this_cfg, label)\n",
    "    sampling_type = c_app[\"sampling\"]  # UAR or POP\n",
    "\n",
    "    # Set solver arguments: levels model\n",
    "    solver_kwargs = {\n",
    "        \"lambdas\": c_app[\"lambdas\"],\n",
    "        \"return_preds\": True,\n",
    "        \"svd_solve\": False,\n",
    "        \"clip_bounds\": np.array([c_app[\"us_bounds_pred\"]]),\n",
    "    }\n",
    "\n",
    "    # Set solver arguments: logs model\n",
    "    solver_kwargs_log = {\n",
    "        \"lambdas\": c_app[\"lambdas\"],\n",
    "        \"return_preds\": True,\n",
    "        \"svd_solve\": False,\n",
    "        \"clip_bounds\": np.array([c_app[\"us_bounds_log_pred\"]]),\n",
    "    }\n",
    "\n",
    "    ## get labels\n",
    "    print(\"Loading labels...\")\n",
    "    this_Y = io.get_Y(this_cfg, c_app[\"colname\"])\n",
    "\n",
    "    # For elevation, below sea level obs need to be clipped to zero for logs (we only perform this for this single test)\n",
    "    if label == \"elevation\":\n",
    "        this_Y[this_Y < 0] = 0\n",
    "\n",
    "    ## merge X and Y accounting for different ordering\n",
    "    ## and the sampling type\n",
    "    print(\"Merging labels and features...\")\n",
    "    this_Y, this_X, this_latlons = parse.merge(\n",
    "        this_Y, X[sampling_type], latlons[sampling_type]\n",
    "    )\n",
    "\n",
    "    ## Drop missing observations as needed\n",
    "    this_X, this_Y, this_latlons = transforms.dropna(\n",
    "        this_X, this_Y, this_latlons, c_app\n",
    "    )\n",
    "\n",
    "    ## Split the data into the training/validation sample vs. test sample\n",
    "    ## (discarding test set for now to keep memory low)\n",
    "    print(\"Splitting training/test...\")\n",
    "    this_X, this_X_test, this_Y, this_Y_test = parse.split_data_train_test(\n",
    "        this_X, this_Y, frac_test=this_cfg.ml_model[\"test_set_frac\"], return_idxs=False\n",
    "    )\n",
    "\n",
    "    ## Create a logged set of outcomes for training/validation sample\n",
    "    this_logY = transforms.log_all(this_Y, c_app)\n",
    "\n",
    "    ## LEVELS model: Train model using ridge regression and 5-fold cross-valiation\n",
    "    ## (number of folds can be adjusted using the argument n_folds)\n",
    "    print(\"Training levels model...\")\n",
    "    kfold_results = solve.kfold_solve(\n",
    "        this_X[subset_n, subset_feat],\n",
    "        this_Y[subset_n],\n",
    "        solve_function=solver,\n",
    "        num_folds=this_cfg.ml_model[\"n_folds\"],\n",
    "        return_model=True,\n",
    "        **solver_kwargs\n",
    "    )\n",
    "    print(\"\")\n",
    "\n",
    "    ## LOGS model: Train model using ridge regression and 5-fold cross-valiation\n",
    "    ## (number of folds can be adjusted using the argument n_folds)\n",
    "    print(\"Training logs model...\")\n",
    "    kfold_results_log = solve.kfold_solve(\n",
    "        this_X[subset_n, subset_feat],\n",
    "        this_logY[subset_n],\n",
    "        solve_function=solver,\n",
    "        num_folds=this_cfg.ml_model[\"n_folds\"],\n",
    "        return_model=True,\n",
    "        **solver_kwargs_log\n",
    "    )\n",
    "    print(\"\")\n",
    "\n",
    "    # Get best predictions from levels model, flatten and calculate R2\n",
    "    (\n",
    "        best_lambda_idx_level,\n",
    "        best_metrics_levels,\n",
    "        best_preds_levels,\n",
    "    ) = ir.interpret_kfold_results(kfold_results, crits=\"r2_score\")\n",
    "    preds_levels = np.vstack(\n",
    "        [solve.y_to_matrix(i) for i in best_preds_levels.squeeze()]\n",
    "    ).squeeze()\n",
    "    truth_levels = np.vstack(\n",
    "        [solve.y_to_matrix(i) for i in kfold_results[\"y_true_test\"].squeeze()]\n",
    "    ).squeeze()\n",
    "    r2_level = metrics.r2_score(truth_levels, preds_levels)\n",
    "\n",
    "    # Get best predictions from logs model, flatten and calculate R2\n",
    "    best_lambda_idx_log, best_metrics_log, best_preds_log = ir.interpret_kfold_results(\n",
    "        kfold_results_log, crits=\"r2_score\"\n",
    "    )\n",
    "    preds_log = np.vstack(\n",
    "        [solve.y_to_matrix(i) for i in best_preds_log.squeeze()]\n",
    "    ).squeeze()\n",
    "    truth_log = np.vstack(\n",
    "        [solve.y_to_matrix(i) for i in kfold_results_log[\"y_true_test\"].squeeze()]\n",
    "    ).squeeze()\n",
    "    r2_log = metrics.r2_score(truth_log, preds_log)\n",
    "\n",
    "    ## Store the R2s\n",
    "    resultsDict[label] = [r2_log, r2_level]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print and save a table of the logs and levels R2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format output\n",
    "out = pd.DataFrame.from_dict(\n",
    "    resultsDict, orient=\"index\", columns=[\"r2_log\", \"r2_level\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get save path\n",
    "if (subset_n != slice(None)) or (subset_feat != slice(None)):\n",
    "    subset_str = \"_subset\"\n",
    "else:\n",
    "    subset_str = \"\"\n",
    "\n",
    "fn = out_dir / f\"LogsVsLevels{subset_str}.csv\"\n",
    "\n",
    "if (not overwrite) and os.path.isfile(fn):\n",
    "    raise OVERWRITE_EXCEPTION\n",
    "\n",
    "# save\n",
    "out.to_csv(fn, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "meaning-jacket",
   "metadata": {},
   "source": [
    "# Tables S5/7: Comparison of MOSAIKS, fine-tuned ResNet-18, pre-trained ResNet-152, and hybrid ResNet-18/MOSAIKS models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-colonial",
   "metadata": {},
   "source": [
    "This notebook produces a table comparing MOSAIKS to alternative SIML approaches:\n",
    "\n",
    "- A fine-tuned ResNet-18 CNN\n",
    "- A pre-trained ResNet-152 CNN\n",
    "- A hybrid ResNet-18/MOSAIKS model\n",
    "\n",
    "The training of the fine-tuned ResNet-18 and pre-trained ResNet-152 models take place in the [train_CNN.py](../../Fig3_diagnostics/train_CNN.py) and [run_pretrained_resnet_regressions.ipynb](../../Fig3_diagnostics/run_pretrained_resnet_regressions.ipynb) files, so this notebook simply inputs the out-of-sample predictions from these previously-trained models. The majority of this notebook is dedicated to creating and training the hybrid model. This model consists of concatenating the last hidden layer of the ResNet-18 model (512 features) with MOSAIKS features (8192 features), and running a ridge regression on the concatenated feature set (allowing for differential regularization parameters on the two sources). Because the CNN features have already been trained on our entire 80k training+validation set, we train on this 80k, choose our hyperparameters using 10k of the remaining 20k (all 20k of which are reserved for testing in all other analyses), and report performance on the final 10k. To ensure comparability of results, we also re-evaluate both the pure MOSAIKS model and the pure ResNet-18 model trained and evaluated on the same sets. The final output table contains: (1) the original MOSAIKS test set results, (2) the original CNN test set results, (3 and 4) the corresponding results from both models on the harmonized 10k test set, (5) the hybrid model results from this harmonized test set, and (6) the pretrained ResNet-152 model results. The latter is not re-tested on the harmonized 10k test set because its performance is not close to that of the other candidate models. These results are used to populate Supplementary Materials Tables S5 and S7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ccb143-6311-4caa-ba94-dae47b92c332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "derived-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "from mosaiks import config as c\n",
    "from mosaiks import transforms as m_transforms\n",
    "from mosaiks.solve import cnn\n",
    "from mosaiks.solve import data_parser as parse\n",
    "from mosaiks.utils import io\n",
    "from scipy.linalg import LinAlgWarning\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from threadpoolctl import threadpool_limits\n",
    "from torch.nn import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lyric-italic",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "fixed_lambda = False\n",
    "overwrite = None\n",
    "\n",
    "SUBSET_N = None\n",
    "SUBSET_FEAT = None\n",
    "OVERWRITE_CNN_FEAT = False\n",
    "NUM_THREADS = None\n",
    "LABELS_TO_RUN = \"all\"\n",
    "\n",
    "L_CNN = np.logspace(-8, 6, 15)\n",
    "L_RCF = np.logspace(-2, 5, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "settled-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUM_THREADS is not None:\n",
    "    threadpool_limits(NUM_THREADS)\n",
    "    os.environ[\"NUMBA_NUM_THREADS\"] = str(NUM_THREADS)\n",
    "if overwrite is None:\n",
    "    overwrite = os.getenv(\"MOSAIKS_OVERWRITE\", False)\n",
    "if LABELS_TO_RUN == \"all\":\n",
    "    LABELS_TO_RUN = c.app_order\n",
    "\n",
    "subset_str = \"\"\n",
    "if (SUBSET_N is not None) or (SUBSET_FEAT is not None):\n",
    "    subset_str = \"_subset\"\n",
    "\n",
    "save_patt = join(\n",
    "    \"{save_dir}\",\n",
    "    \"model_cnnHybrid_{label}_{variable}_CONTUS_16_640_{sampling}_\"\n",
    "    f\"{c.sampling['n_samples']}_{c.sampling['seed']}_\"\n",
    "    f\"{c.features['random']['patch_size']}_\"\n",
    "    f\"{c.features['random']['seed']}{subset_str}.pickle\",\n",
    ")\n",
    "\n",
    "out_dir = Path(c.res_dir) / \"tables\" / \"TableS5\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-desperate",
   "metadata": {},
   "source": [
    "## Create CNN feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "iraqi-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(task, remove_fc=True):\n",
    "    test_r2, cnn_out = io.load_cnn_performance(task, c, extra_return=[\"model\"])\n",
    "    if remove_fc:\n",
    "        cnn_out.fc = Sequential()\n",
    "    return test_r2, cnn_out\n",
    "\n",
    "\n",
    "def get_cnn_ids(task):\n",
    "    model = load(\n",
    "        Path(c.data_dir) / \"output\" / \"cnn_comparison\" / f\"resnet18_{task}.pickle\"\n",
    "    )\n",
    "    return {\"test\": model[\"ids_test\"], \"train\": model[\"ids_train\"]}\n",
    "\n",
    "\n",
    "def load_cnn_feats(task, c):\n",
    "    sample = getattr(c, task)[\"sampling\"]\n",
    "    subgrid_path = c.grid_paths[sample]\n",
    "    cnn_ids = np.load(subgrid_path)[\"ID\"].astype(str)\n",
    "    cnn_feat_path = Path(c.features_dir) / f\"CONTUS_{sample}_resnet18_{task}.npy\"\n",
    "    cnn_feats = np.load(cnn_feat_path)\n",
    "    cnn_feats = pd.DataFrame(\n",
    "        cnn_feats, index=cnn_ids, columns=[f\"XR_{i}\" for i in range(cnn_feats.shape[1])]\n",
    "    )\n",
    "    return cnn_feats\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCategory:\n",
    "    X: np.array\n",
    "    Y: np.array\n",
    "    latlons: np.array\n",
    "    ids: np.array\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AllData:\n",
    "    train: DataCategory\n",
    "    val: DataCategory\n",
    "    test: DataCategory\n",
    "\n",
    "\n",
    "def split_data(task, X, latlons, c):\n",
    "\n",
    "    c = io.get_filepaths(c, task)\n",
    "    c_app = getattr(c, task)\n",
    "\n",
    "    Y = io.get_Y(c, c_app[\"colname\"])\n",
    "\n",
    "    # merge\n",
    "    Y, X, latlons, ids = parse.merge(\n",
    "        Y, X, latlons, pd.Series(Y.index.values, index=Y.index)\n",
    "    )\n",
    "\n",
    "    # drop nulls\n",
    "    Y, valid = m_transforms.dropna_Y(Y, task)\n",
    "    X, latlons, ids = map(lambda x: x[valid], (X, latlons, ids))\n",
    "\n",
    "    cnn_ids = get_cnn_ids(task)\n",
    "    in_test = np.isin(ids, cnn_ids[\"test\"])\n",
    "    in_train = np.isin(ids, cnn_ids[\"train\"])\n",
    "    X_test, Y_test, latlons_test, ids_test = map(\n",
    "        lambda x: x[in_test], (X, Y, latlons, ids)\n",
    "    )\n",
    "    X_train, Y_train, latlons_train, ids_train = map(\n",
    "        lambda x: x[in_train], (X, Y, latlons, ids)\n",
    "    )\n",
    "\n",
    "    # apply transform\n",
    "    X, Y, latlons = getattr(m_transforms, f\"transform_{task}\")(\n",
    "        X, Y, latlons, c_app[\"logged\"]\n",
    "    )\n",
    "\n",
    "    # split train/test to match CNN\n",
    "    cnn_test_ids = get_cnn_ids(task)\n",
    "    in_test = np.isin(ids, cnn_test_ids[\"test\"])\n",
    "    in_train = np.isin(ids, cnn_test_ids[\"train\"])\n",
    "    X_test, Y_test, latlons_test, ids_test = map(\n",
    "        lambda x: x[in_test], (X, Y, latlons, ids)\n",
    "    )\n",
    "    X_train, Y_train, latlons_train, ids_train = map(\n",
    "        lambda x: x[in_train], (X, Y, latlons, ids)\n",
    "    )\n",
    "\n",
    "    # split test set in half for validation and test set\n",
    "    rng = np.random.default_rng(c.ml_model[\"seed\"])\n",
    "    val_ixs = rng.choice(Y_test.shape[0], size=int(Y_test.shape[0] / 2), replace=False)\n",
    "    all_ixs = np.arange(Y_test.shape[0])\n",
    "    test_ixs = all_ixs[~np.isin(all_ixs, val_ixs)]\n",
    "    X_val, Y_val, latlons_val, ids_val = map(\n",
    "        lambda x: x[val_ixs], (X_test, Y_test, latlons_test, ids_test)\n",
    "    )\n",
    "    X_test, Y_test, latlons_test, ids_test = map(\n",
    "        lambda x: x[test_ixs], (X_test, Y_test, latlons_test, ids_test)\n",
    "    )\n",
    "\n",
    "    # subset\n",
    "    X_train = X_train[slice(SUBSET_N), slice(SUBSET_FEAT)]\n",
    "    X_val = X_val[:, slice(SUBSET_FEAT)]\n",
    "    X_test = X_test[:, slice(SUBSET_FEAT)]\n",
    "    Y_train = Y_train[slice(SUBSET_N)]\n",
    "    latlons_train = latlons_train[slice(SUBSET_N)]\n",
    "\n",
    "    train = DataCategory(X_train, Y_train, latlons_train, ids_train)\n",
    "    val = DataCategory(X_val, Y_val, latlons_val, ids_val)\n",
    "    test = DataCategory(X_test, Y_test, latlons_test, ids_test)\n",
    "\n",
    "    return AllData(train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "minimal-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "models[\"POP\"] = {}\n",
    "models[\"UAR\"] = {}\n",
    "for i in LABELS_TO_RUN:\n",
    "    sample = getattr(c, i)[\"sampling\"]\n",
    "    _, models[sample][i] = get_model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "specific-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in [\"UAR\", \"POP\"]:\n",
    "    to_write = {\n",
    "        task: model\n",
    "        for task, model in models[sample].items()\n",
    "        if OVERWRITE_CNN_FEAT\n",
    "        or (\n",
    "            not (\n",
    "                Path(c.features_dir) / f\"CONTUS_{sample}_resnet18_{task}.npy\"\n",
    "            ).is_file()\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # skip if nothing to overwrite\n",
    "    if len(to_write) == 0:\n",
    "        continue\n",
    "\n",
    "    outputs = {i: [] for i in to_write}\n",
    "    # get paths\n",
    "    subgrid_path = c.grid_paths[sample]\n",
    "    img_dir = Path(c.data_dir) / \"raw\" / \"imagery\" / f\"CONTUS_{sample}\"\n",
    "    grid = np.load(subgrid_path)\n",
    "    y = grid[\"lat\"]  # nonsense y var\n",
    "    ids = grid[\"ID\"].astype(str)\n",
    "\n",
    "    # configure dataloader\n",
    "    dl = cnn.get_dataloader(img_dir, y, ids, shuffle=False, subset=SUBSET_N)\n",
    "\n",
    "    features = []\n",
    "    for _, img, _ in dl:\n",
    "        for task, model in to_write.items():\n",
    "            outputs[task].append(model(img).detach().numpy())\n",
    "\n",
    "    for task in outputs:\n",
    "        outputs[task] = np.concatenate(outputs[task], axis=0)\n",
    "        np.save(\n",
    "            Path(c.features_dir) / f\"CONTUS_{sample}_resnet18_{task}.npy\",\n",
    "            outputs[task],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-solid",
   "metadata": {},
   "source": [
    "## Run regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blond-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, latlons_all = {}, {}\n",
    "X_all[\"UAR\"], latlons_all[\"UAR\"] = io.get_X_latlon(c, \"UAR\")\n",
    "X_all[\"POP\"], latlons_all[\"POP\"] = io.get_X_latlon(c, \"POP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "quiet-lawrence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running regressions for task 1/7: treecover\n",
      "...Loading data\n",
      "...Merging RCF/CNN\n",
      "...Splitting train/test\n",
      "...Testing CNN lambda 1 / 1\n",
      "......Testing RCF lambda 1 / 1\n",
      "Running regressions for task 2/7: elevation\n",
      "...Loading data\n",
      "...Merging RCF/CNN\n",
      "...Splitting train/test\n",
      "...Testing CNN lambda 1 / 1\n",
      "......Testing RCF lambda 1 / 1\n",
      "Running regressions for task 3/7: population\n",
      "...Loading data\n",
      "...Merging RCF/CNN\n",
      "...Splitting train/test\n",
      "...Testing CNN lambda 1 / 1\n",
      "......Testing RCF lambda 1 / 1\n",
      "Running regressions for task 4/7: nightlights\n",
      "...Loading data\n",
      "...Merging RCF/CNN\n",
      "...Splitting train/test\n",
      "...Testing CNN lambda 1 / 1\n",
      "......Testing RCF lambda 1 / 1\n",
      "Running regressions for task 5/7: income\n",
      "...Loading data\n",
      "...Merging RCF/CNN\n",
      "...Splitting train/test\n",
      "...Testing CNN lambda 1 / 1\n",
      "......Testing RCF lambda 1 / 1\n",
      "Running regressions for task 6/7: roads\n",
      "...Loading data\n",
      "...Merging RCF/CNN\n",
      "...Splitting train/test\n",
      "...Testing CNN lambda 1 / 1\n",
      "......Testing RCF lambda 1 / 1\n",
      "Running regressions for task 7/7: housing\n",
      "...Loading data\n",
      "...Merging RCF/CNN\n",
      "...Splitting train/test\n",
      "...Testing CNN lambda 1 / 1\n",
      "......Testing RCF lambda 1 / 1\n"
     ]
    }
   ],
   "source": [
    "# run on all tasks\n",
    "bad_tasks = []\n",
    "for tx, task in enumerate(LABELS_TO_RUN):\n",
    "    print(f\"Running regressions for task {tx+1}/{len(LABELS_TO_RUN)}: {task}\")\n",
    "\n",
    "    # get general paths\n",
    "    c = io.get_filepaths(c, task)\n",
    "    c_app = getattr(c, task)\n",
    "    sample = c_app[\"sampling\"]\n",
    "\n",
    "    # Get save path\n",
    "    save_path = Path(\n",
    "        save_patt.format(\n",
    "            save_dir=c.fig_dir_prim,\n",
    "            label=task,\n",
    "            variable=c_app[\"variable\"],\n",
    "            sampling=sample,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    best_lr = None\n",
    "    best_lc = None\n",
    "    this_l_cnn = L_CNN.copy()\n",
    "    this_l_rcf = L_RCF.copy()\n",
    "    this_l_cnn_save = L_CNN.copy()\n",
    "    this_l_rcf_save = L_RCF.copy()\n",
    "    already_run_cnn = []\n",
    "    already_run_rcf = []\n",
    "    ill_conditioned_rcf = []\n",
    "    ill_conditioned_cnn = []\n",
    "    if save_path.is_file():\n",
    "        model = load(save_path)\n",
    "        if fixed_lambda:\n",
    "            params = model.get_params()\n",
    "            l_rat = params[\"transform__kw_args\"][\"l_rat\"]\n",
    "            lr = params[\"regress__regressor__alpha\"]\n",
    "            lc = lr / l_rat\n",
    "            this_l_cnn = np.array([lc])\n",
    "            this_l_rcf = np.array([lr])\n",
    "            hp_hits_boundary_prev = model.hp_hits_boundary\n",
    "        elif not overwrite:\n",
    "            already_run_cnn = model.lambdas_cnn\n",
    "            already_run_rcf = model.lambdas_rcf\n",
    "            best_lr = model.best_lr\n",
    "            best_lc = model.best_lc\n",
    "            ill_conditioned_cnn = getattr(model, \"ill_conditioned_cnn\", [])\n",
    "            ill_conditioned_rcf = getattr(model, \"ill_conditioned_rcf\", [])\n",
    "            if (\n",
    "                np.isin(this_l_cnn, already_run_cnn).all()\n",
    "                and np.isin(this_l_rcf, already_run_rcf).all()\n",
    "            ):\n",
    "                print(\n",
    "                    f\"{task} task output file already exists and no new \"\n",
    "                    \"hyperparameters are being tested.\"\n",
    "                )\n",
    "            this_l_cnn_save = np.sort(\n",
    "                np.unique(np.concatenate((this_l_cnn_save, already_run_cnn)))\n",
    "            )\n",
    "            this_l_rcf_save = np.sort(\n",
    "                np.unique(np.concatenate((this_l_rcf_save, already_run_rcf)))\n",
    "            )\n",
    "            this_l_cnn = this_l_cnn[~np.isin(this_l_cnn, ill_conditioned_cnn)]\n",
    "            this_l_rcf = this_l_rcf[~np.isin(this_l_rcf, ill_conditioned_rcf)]\n",
    "\n",
    "    # load X\n",
    "    print(\"...Loading data\")\n",
    "    X, latlons = X_all[sample], latlons_all[sample]\n",
    "\n",
    "    # load cnn features\n",
    "    cnn_feats = load_cnn_feats(task, c)\n",
    "    n_cnn_feat = cnn_feats.shape[1]\n",
    "\n",
    "    # merge with RCF features\n",
    "    print(\"...Merging RCF/CNN\")\n",
    "    X = X.join(cnn_feats)\n",
    "\n",
    "    # load y and split all data into train/val/test based on CNN test set\n",
    "    print(\"...Splitting train/test\")\n",
    "    data = split_data(task, X, latlons, c)\n",
    "\n",
    "    # train model\n",
    "    best_score = -np.inf\n",
    "    ridge_regr = cnn.get_bounded_ridge_regressor(c, task)\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"transform\",\n",
    "                cnn.get_hybrid_adjust_weights_transformer(n_cnn_feat=n_cnn_feat),\n",
    "            ),\n",
    "            (\"regress\", ridge_regr),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"error\", category=LinAlgWarning)\n",
    "\n",
    "        if not fixed_lambda:\n",
    "            # test for ill conditioning within CNN and skip these hp's if so to save time in\n",
    "            # 2D grid search\n",
    "            print(\"...Testing CNN only model\")\n",
    "            for lcx, lc in enumerate(this_l_cnn):\n",
    "                ridge_regr.set_params(regressor__alpha=lc)\n",
    "                try:\n",
    "                    ridge_regr.fit(data.train.X[:, -n_cnn_feat:], data.train.Y)\n",
    "                    score = ridge_regr.score(data.val.X[:, -n_cnn_feat:], data.val.Y)\n",
    "                    if score > best_score:\n",
    "                        best_pipe = ridge_regr\n",
    "                        best_lc = lc\n",
    "                        best_lr = None\n",
    "                        best_score = score\n",
    "                    break\n",
    "                except LinAlgWarning:\n",
    "                    pass\n",
    "            if lcx != 0:\n",
    "                print(\n",
    "                    f\"......Dropping first {lcx} CNN lambdas in grid search due to \"\n",
    "                    \"ill-conditioning\"\n",
    "                )\n",
    "            this_l_cnn = this_l_cnn[lcx:]\n",
    "            ill_conditioned_cnn += list(this_l_cnn[:lcx])\n",
    "\n",
    "            # test for ill conditioning within RCF and skip these hp's if so\n",
    "            print(\"...Testing RCF only model\")\n",
    "            for lrx, lr in enumerate(this_l_rcf):\n",
    "                ridge_regr.set_params(regressor__alpha=lr)\n",
    "                try:\n",
    "                    ridge_regr.fit(data.train.X[:, :-n_cnn_feat], data.train.Y)\n",
    "                    score = ridge_regr.score(data.val.X[:, :-n_cnn_feat], data.val.Y)\n",
    "                    if score > best_score:\n",
    "                        best_pipe = ridge_regr\n",
    "                        best_lc = None\n",
    "                        best_lr = lr\n",
    "                        best_score = score\n",
    "                    break\n",
    "                except LinAlgWarning:\n",
    "                    pass\n",
    "            if lrx != 0:\n",
    "                print(\n",
    "                    f\"......Dropping first {lrx} RCF lambdas in grid search due to \"\n",
    "                    \"ill-conditioning\"\n",
    "                )\n",
    "            this_l_rcf = this_l_rcf[lrx:]\n",
    "            ill_conditioned_rcf += list(this_l_cnn[:lrx])\n",
    "\n",
    "        # now do grid search over remaining sets of both hyperparameters\n",
    "        for lcx, lc in enumerate(this_l_cnn):\n",
    "            print(f\"...Testing CNN lambda {lcx+1} / {len(this_l_cnn)}\")\n",
    "            for lrx, lr in enumerate(this_l_rcf):\n",
    "                print(f\"......Testing RCF lambda {lrx+1} / {len(this_l_rcf)}\", end=\"\")\n",
    "                if lr in already_run_rcf and lc in already_run_cnn:\n",
    "                    print(\".........skipping b/c hp set already run in previous search\")\n",
    "                    continue\n",
    "                l_rat = lr / lc\n",
    "                this_pipe = pipe.set_params(\n",
    "                    transform__kw_args={\n",
    "                        \"l_rat\": l_rat,\n",
    "                        \"n_cnn_feat\": n_cnn_feat,\n",
    "                    },\n",
    "                    regress__regressor__alpha=lr,\n",
    "                )\n",
    "\n",
    "                # ignore models that raise an ill-conditioned warning\n",
    "                try:\n",
    "                    this_pipe.fit(data.train.X, data.train.Y)\n",
    "                except LinAlgWarning:\n",
    "                    print(\"...skipped due to ill-condition warning\")\n",
    "                    continue\n",
    "                print(\"\")\n",
    "\n",
    "                score = this_pipe.score(data.val.X, data.val.Y)\n",
    "                if score > best_score:\n",
    "                    best_pipe = this_pipe\n",
    "                    best_lc = lc\n",
    "                    best_lr = lr\n",
    "                    best_score = score\n",
    "\n",
    "    if best_lr is None:\n",
    "        bad_tasks.append(task)\n",
    "        continue\n",
    "\n",
    "    # refit model\n",
    "    best_pipe.set_params(\n",
    "        transform__kw_args={\n",
    "            \"l_rat\": best_lr / best_lc,\n",
    "            \"n_cnn_feat\": n_cnn_feat,\n",
    "        },\n",
    "        regress__regressor__alpha=best_lr,\n",
    "    )\n",
    "    best_pipe.fit(data.train.X, data.train.Y)\n",
    "    best_pipe.val_r2 = best_score\n",
    "    best_pipe.test_r2 = best_pipe.score(data.test.X, data.test.Y)\n",
    "    best_pipe.lambdas_cnn = this_l_cnn_save\n",
    "    best_pipe.lambdas_rcf = this_l_rcf_save\n",
    "    best_pipe.best_lc = best_lc\n",
    "    best_pipe.best_lr = best_lr\n",
    "    best_pipe.ill_conditioned_cnn = ill_conditioned_cnn\n",
    "    best_pipe.ill_conditioned_rcf = ill_conditioned_rcf\n",
    "    if fixed_lambda:\n",
    "        best_pipe.hp_hits_boundary = hp_hits_boundary_prev\n",
    "    else:\n",
    "        best_pipe.hp_hits_boundary = {\n",
    "            \"cnn\": {\n",
    "                \"upper\": best_lc == this_l_cnn_save[-1],\n",
    "                \"lower\": best_lc == this_l_cnn_save[0],\n",
    "            },\n",
    "            \"rcf\": {\n",
    "                \"upper\": best_lr == this_l_rcf_save[-1],\n",
    "                \"lower\": best_lr == this_l_rcf_save[0],\n",
    "            },\n",
    "        }\n",
    "\n",
    "    # save model\n",
    "    dump(best_pipe, save_path)\n",
    "\n",
    "\n",
    "if len(bad_tasks) > 0:\n",
    "    raise ValueError(\n",
    "        f\"No non-ill-conditioned hyperparameter values available for tasks: {bad_tasks}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-bahrain",
   "metadata": {},
   "source": [
    "## Validate that best chosen model is not hitting hyperparameter bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "iraqi-utilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treecover\n",
      "{'cnn': {'upper': False, 'lower': False}, 'rcf': {'upper': False, 'lower': False}}\n",
      "1e-06 1.0 0.9425213101647336\n",
      "\n",
      "elevation\n",
      "{'cnn': {'upper': False, 'lower': False}, 'rcf': {'upper': False, 'lower': False}}\n",
      "100.0 1.0 0.8071998090934125\n",
      "\n",
      "population\n",
      "{'cnn': {'upper': False, 'lower': False}, 'rcf': {'upper': False, 'lower': False}}\n",
      "1e-06 1.0 0.8131466254822377\n",
      "\n",
      "nightlights\n",
      "{'cnn': {'upper': False, 'lower': False}, 'rcf': {'upper': False, 'lower': False}}\n",
      "10000.0 1.0 0.9006131808056724\n",
      "\n",
      "income\n",
      "{'cnn': {'upper': False, 'lower': False}, 'rcf': {'upper': False, 'lower': False}}\n",
      "99999.99999999999 1.0 0.5061013970456906\n",
      "\n",
      "roads\n",
      "{'cnn': {'upper': False, 'lower': False}, 'rcf': {'upper': False, 'lower': False}}\n",
      "10000.0 1.0 0.5922103197828472\n",
      "\n",
      "housing\n",
      "{'cnn': {'upper': False, 'lower': False}, 'rcf': {'upper': False, 'lower': False}}\n",
      "99999.99999999999 1.0 0.665529435859699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task in c.app_order:\n",
    "\n",
    "    print(task)\n",
    "\n",
    "    # get general paths\n",
    "    c = io.get_filepaths(c, task)\n",
    "    c_app = getattr(c, task)\n",
    "    sample = c_app[\"sampling\"]\n",
    "    subgrid_path = c.grid_paths[sample]\n",
    "\n",
    "    # Get save path\n",
    "    save_path = Path(\n",
    "        save_patt.format(\n",
    "            save_dir=c.fig_dir_prim,\n",
    "            label=task,\n",
    "            variable=c_app[\"variable\"],\n",
    "            sampling=c_app[\"sampling\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    best_pipe = load(save_path)\n",
    "\n",
    "    print(best_pipe.hp_hits_boundary)\n",
    "    print(best_pipe.best_lc, best_pipe.best_lr, best_pipe.test_r2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-physiology",
   "metadata": {},
   "source": [
    "## Get MOSAIKS predictions using the same train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "after-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mosaiks output pattern\n",
    "mosaiks_patt = join(\n",
    "    \"{save_dir}\",\n",
    "    \"outcomes_scatter_obsAndPred_{label}_{variable}_CONTUS_16_640_{sampling}_\"\n",
    "    f\"{c.sampling['n_samples']}_{c.sampling['seed']}_random_features_\"\n",
    "    f\"{c.features['random']['patch_size']}_\"\n",
    "    f\"{c.features['random']['seed']}{subset_str}.data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "packed-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for task: treecover...\n",
      "...Loading data\n",
      "...Retraining and predicting using MOSAIKS\n",
      "...predicting using Resnet\n",
      "Running for task: elevation...\n",
      "...Loading data\n",
      "...Retraining and predicting using MOSAIKS\n",
      "...predicting using Resnet\n",
      "Running for task: population...\n",
      "...Loading data\n",
      "...Retraining and predicting using MOSAIKS\n",
      "...predicting using Resnet\n",
      "Running for task: nightlights...\n",
      "...Loading data\n",
      "...Retraining and predicting using MOSAIKS\n",
      "...predicting using Resnet\n",
      "Running for task: income...\n",
      "...Loading data\n",
      "...Retraining and predicting using MOSAIKS\n",
      "...predicting using Resnet\n",
      "Running for task: roads...\n",
      "...Loading data\n",
      "...Retraining and predicting using MOSAIKS\n",
      "...predicting using Resnet\n",
      "Running for task: housing...\n",
      "...Loading data\n",
      "...Retraining and predicting using MOSAIKS\n",
      "...predicting using Resnet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mosaiks</th>\n",
       "      <th>mosaiks_10ktest</th>\n",
       "      <th>resnet18</th>\n",
       "      <th>resnet18_10ktest</th>\n",
       "      <th>hybrid_10ktest</th>\n",
       "      <th>pretrained</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>treecover</th>\n",
       "      <td>0.913321</td>\n",
       "      <td>0.894866</td>\n",
       "      <td>0.941586</td>\n",
       "      <td>0.940609</td>\n",
       "      <td>0.942521</td>\n",
       "      <td>0.657796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elevation</th>\n",
       "      <td>0.680964</td>\n",
       "      <td>0.681033</td>\n",
       "      <td>0.799261</td>\n",
       "      <td>0.803045</td>\n",
       "      <td>0.8072</td>\n",
       "      <td>0.315231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>0.724783</td>\n",
       "      <td>0.714906</td>\n",
       "      <td>0.801206</td>\n",
       "      <td>0.808143</td>\n",
       "      <td>0.813147</td>\n",
       "      <td>0.289154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nightlights</th>\n",
       "      <td>0.846592</td>\n",
       "      <td>0.849689</td>\n",
       "      <td>0.890472</td>\n",
       "      <td>0.89124</td>\n",
       "      <td>0.900613</td>\n",
       "      <td>0.475350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income</th>\n",
       "      <td>0.452314</td>\n",
       "      <td>0.4535</td>\n",
       "      <td>0.473877</td>\n",
       "      <td>0.46613</td>\n",
       "      <td>0.506101</td>\n",
       "      <td>0.070850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roads</th>\n",
       "      <td>0.533398</td>\n",
       "      <td>0.532579</td>\n",
       "      <td>0.575179</td>\n",
       "      <td>0.579945</td>\n",
       "      <td>0.59221</td>\n",
       "      <td>0.160777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing</th>\n",
       "      <td>0.586562</td>\n",
       "      <td>0.609621</td>\n",
       "      <td>0.495744</td>\n",
       "      <td>0.561459</td>\n",
       "      <td>0.665529</td>\n",
       "      <td>0.011157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mosaiks mosaiks_10ktest  resnet18 resnet18_10ktest  \\\n",
       "task                                                               \n",
       "treecover    0.913321        0.894866  0.941586         0.940609   \n",
       "elevation    0.680964        0.681033  0.799261         0.803045   \n",
       "population   0.724783        0.714906  0.801206         0.808143   \n",
       "nightlights  0.846592        0.849689  0.890472          0.89124   \n",
       "income       0.452314          0.4535  0.473877          0.46613   \n",
       "roads        0.533398        0.532579  0.575179         0.579945   \n",
       "housing      0.586562        0.609621  0.495744         0.561459   \n",
       "\n",
       "            hybrid_10ktest  pretrained  \n",
       "task                                    \n",
       "treecover         0.942521    0.657796  \n",
       "elevation           0.8072    0.315231  \n",
       "population        0.813147    0.289154  \n",
       "nightlights       0.900613    0.475350  \n",
       "income            0.506101    0.070850  \n",
       "roads              0.59221    0.160777  \n",
       "housing           0.665529    0.011157  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.DataFrame(\n",
    "    index=pd.Index(LABELS_TO_RUN, name=\"task\"),\n",
    "    columns=[\n",
    "        \"mosaiks\",\n",
    "        \"mosaiks_10ktest\",\n",
    "        \"resnet18\",\n",
    "        \"resnet18_10ktest\",\n",
    "        \"hybrid_10ktest\",\n",
    "    ],\n",
    ")\n",
    "for task in LABELS_TO_RUN:\n",
    "    print(f\"Running for task: {task}...\")\n",
    "    # get general paths\n",
    "    c = io.get_filepaths(c, task)\n",
    "    c_app = getattr(c, task)\n",
    "    sample = c_app[\"sampling\"]\n",
    "\n",
    "    # Get optimal lambda and test r2\n",
    "    mosaiks_outpath = Path(\n",
    "        mosaiks_patt.format(\n",
    "            save_dir=c.fig_dir_prim,\n",
    "            label=task,\n",
    "            variable=c_app[\"variable\"],\n",
    "            sampling=c_app[\"sampling\"],\n",
    "        )\n",
    "    )\n",
    "    test_model = load(\n",
    "        mosaiks_outpath.parent / mosaiks_outpath.name.replace(\"scatter\", \"testset\")\n",
    "    )\n",
    "    scores.loc[task, \"mosaiks\"] = r2_score(test_model[\"truth\"], test_model[\"preds\"])\n",
    "\n",
    "    best_lambda = load(mosaiks_outpath)[\"best_lambda\"]\n",
    "    assert len(best_lambda) == 1\n",
    "    best_lambda = best_lambda[0]\n",
    "\n",
    "    print(\"...Loading data\")\n",
    "\n",
    "    # load last layer of CNN features and concatenate onto mosaiks in order to split\n",
    "    # the same way\n",
    "    cnn_feats = load_cnn_feats(task, c)\n",
    "    n_cnn_feat = cnn_feats.shape[1]\n",
    "    this_X = X_all[sample].join(cnn_feats, how=\"left\")\n",
    "\n",
    "    # load y and split all data into train/val/test based on CNN test set\n",
    "    print(\"...Splitting train/test\")\n",
    "    data = split_data(task, this_X, latlons_all[sample], c)\n",
    "\n",
    "    print(\"...Retraining and predicting using MOSAIKS\")\n",
    "\n",
    "    # train and evaluate MOSAIKS model on new 80k train/10k test dataset\n",
    "    ridge_regr = Ridge(fit_intercept=False, random_state=0, alpha=best_lambda)\n",
    "    ridge_regr.fit(data.train.X[:, :-n_cnn_feat], data.train.Y)\n",
    "    scores.loc[task, \"mosaiks_10ktest\"] = ridge_regr.score(\n",
    "        data.test.X[:, :-n_cnn_feat], data.test.Y\n",
    "    )\n",
    "\n",
    "    # evaluate RESNET on both 20k (original result) and 10k (harmonized test set)\n",
    "    print(\"...predicting using Resnet\")\n",
    "    test_r2, model = get_model(task, remove_fc=False)\n",
    "    scores.loc[task, \"resnet18\"] = test_r2\n",
    "    weights = model.fc.weight.detach().numpy().T\n",
    "    cnn_pred = np.dot(data.test.X[:, -n_cnn_feat:], weights)\n",
    "\n",
    "    mean = data.train.Y.mean()\n",
    "    std = data.train.Y.std()\n",
    "    cnn_pred = cnn_pred * std + mean\n",
    "\n",
    "    cnn_pred = cnn.clip_bounds(cnn_pred, c_app)\n",
    "    scores.loc[task, \"resnet18_10ktest\"] = r2_score(data.test.Y, cnn_pred)\n",
    "\n",
    "    # load hybrid model results\n",
    "    hyb_outpath = Path(\n",
    "        save_patt.format(\n",
    "            save_dir=c.fig_dir_prim,\n",
    "            label=task,\n",
    "            variable=c_app[\"variable\"],\n",
    "            sampling=c_app[\"sampling\"],\n",
    "        )\n",
    "    )\n",
    "    scores.loc[task, \"hybrid_10ktest\"] = load(hyb_outpath).test_r2\n",
    "\n",
    "# merge pretrained results\n",
    "pretrained_outpath = (\n",
    "    Path(c.data_dir)\n",
    "    / \"output\"\n",
    "    / \"cnn_comparison\"\n",
    "    / \"TestSetR2_resnet152_1e5_pretrained.csv\"\n",
    ")\n",
    "pretrained = pd.read_csv(\n",
    "    pretrained_outpath, header=None, index_col=0, names=[\"pretrained\"]\n",
    ")\n",
    "pretrained.index.name = \"task\"\n",
    "scores = scores.join(pretrained)\n",
    "\n",
    "scores = pd.DataFrame(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "another-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.to_csv(out_dir / \"MOSAIKS_vs_CNN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-leader",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mosaiks-env",
   "language": "python",
   "name": "mosaiks-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

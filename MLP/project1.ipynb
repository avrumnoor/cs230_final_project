{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import numpy as np\n",
    "from numpy import vstack\n",
    "from numpy import sqrt\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import pickle as pkl\n",
    "\n",
    "import io as b_io\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "print_every = 1000\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75235456\n",
      "<class 'bytes'>\n",
      "2296\n",
      "(2296, 8192)\n"
     ]
    }
   ],
   "source": [
    "# Load feature vectors\n",
    "out_file = \"./data/CONTUS_POP_self_storage.pkl\"\n",
    "with open(out_file, \"rb\") as f:\n",
    "    data = np.load(f, allow_pickle = True)\n",
    "print(len(data[\"X\"]))\n",
    "print(type(data[\"X\"]))\n",
    "print(len(data[\"latlon\"]))\n",
    "temp = b_io.BytesIO(data[\"X\"])\n",
    "features = np.load(temp, allow_pickle=True)\n",
    "print(features.shape)\n",
    "latlons_samp = data[\"latlon\"]\n",
    "ids_x = data[\"ids_X\"]\n",
    "\n",
    "X = pd.DataFrame(features, index = ids_x)\n",
    "X.index.name = \"foo\"\n",
    "X = X.sort_values(\"foo\", ascending=True)\n",
    "lls = pd.DataFrame(latlons_samp, index=ids_x, columns=[\"lat\", \"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "data_train=pd.read_csv('./data/train_df_with_census_data.tsv',sep='\\t')\n",
    "data_test=pd.read_csv('./data/test_id_df_with_census_data.tsv',sep='\\t')\n",
    "data_test2=pd.read_csv('./data/test_ood_df_with_census_data.tsv',sep='\\t')\n",
    "labels_dataset = [data_train, data_test, data_test2]\n",
    "y_dataset = pd.concat(labels_dataset).sort_values(\"places_id\", ascending=True)\n",
    "y_labels = y_dataset.groupby([\"places_id\"]).mean()\n",
    "y = y_labels[\"price\"]\n",
    "# Normalize Inputs\n",
    "# y = (y_labels[\"price\"] - y_labels[\"price\"].mean())/y_labels[\"price\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "NUM_EXAMPLES = len(X)\n",
    "NUM_TRAIN = int(NUM_EXAMPLES * 0.8)\n",
    "NUM_VAL = (NUM_EXAMPLES - NUM_TRAIN) // 2\n",
    "NUM_TEST = NUM_EXAMPLES - NUM_VAL - NUM_TRAIN\n",
    "\n",
    "X_train_np = X[0:NUM_TRAIN].values\n",
    "X_val_np = X[NUM_TRAIN:NUM_TRAIN + NUM_VAL].values\n",
    "X_test_np = X[NUM_TRAIN + NUM_VAL:].values\n",
    "X_train = torch.from_numpy(X_train_np.astype(np.float32))\n",
    "X_val = torch.from_numpy(X_val_np.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test_np.astype(np.float32))\n",
    "\n",
    "y_train_np = y[0:NUM_TRAIN].values\n",
    "y_val_np = y[NUM_TRAIN:NUM_TRAIN + NUM_VAL].values\n",
    "y_test_np = y[NUM_TRAIN + NUM_VAL:].values\n",
    "y_train = torch.from_numpy(y_train_np.astype(np.float32))\n",
    "y_val = torch.from_numpy(y_val_np.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test_np.astype(np.float32))\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train) \n",
    "loader_train = DataLoader(train_dataset, batch_size=68)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val) \n",
    "loader_val = DataLoader(val_dataset, batch_size=46)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test) \n",
    "loader_test = DataLoader(test_dataset, batch_size=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "no_input_features = X_train_np.shape[1]\n",
    "class Three_Layer_Network(torch.nn.Module):\n",
    "    def __init__(self,no_input_features):\n",
    "        super(Three_Layer_Network,self).__init__()\n",
    "        self.layer1=nn.Linear(no_input_features,10)\n",
    "        nn.init.xavier_uniform_(self.layer1.weight)\n",
    "        self.layer2=nn.ReLU()\n",
    "        self.layer3=nn.Linear(10,8)\n",
    "        nn.init.xavier_uniform_(self.layer3.weight)\n",
    "        self.layer4=nn.ReLU()\n",
    "        self.layer5=nn.Linear(8,1)\n",
    "        nn.init.xavier_uniform_(self.layer5.weight)\n",
    "    def forward(self,x):\n",
    "       y_predicted=self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x)))))\n",
    "       return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Evaluation Metric\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate mse\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            criterion = nn.MSELoss()\n",
    "            scores = model(x)\n",
    "            loss = criterion(torch.reshape(scores, (68,)), y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                val_loss, r2 = evaluate_model(loader_val, model)\n",
    "                print('Validation loss = %.4f' % (val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 44211.5039\n",
      "Validation loss = 34997.4141\n",
      "Iteration 0, loss = 6481.4214\n",
      "Validation loss = 6973.7227\n",
      "Iteration 0, loss = 5792.6948\n",
      "Validation loss = 6685.8555\n",
      "Iteration 0, loss = 5729.4678\n",
      "Validation loss = 6606.5068\n",
      "Iteration 0, loss = 5698.3262\n",
      "Validation loss = 6576.1631\n",
      "Iteration 0, loss = 5652.6265\n",
      "Validation loss = 6546.0400\n",
      "Iteration 0, loss = 5609.7915\n",
      "Validation loss = 6520.4141\n",
      "Iteration 0, loss = 5575.4092\n",
      "Validation loss = 6499.6499\n",
      "Iteration 0, loss = 5548.3540\n",
      "Validation loss = 6482.6650\n",
      "Iteration 0, loss = 5525.8584\n",
      "Validation loss = 6468.4277\n",
      "Iteration 0, loss = 5506.3281\n",
      "Validation loss = 6456.4937\n",
      "Iteration 0, loss = 5489.5488\n",
      "Validation loss = 6446.8022\n",
      "Iteration 0, loss = 5475.4805\n",
      "Validation loss = 6439.1528\n",
      "Iteration 0, loss = 5463.7925\n",
      "Validation loss = 6433.1816\n",
      "Iteration 0, loss = 5454.0698\n",
      "Validation loss = 6428.5156\n",
      "Iteration 0, loss = 5445.9146\n",
      "Validation loss = 6424.8306\n",
      "Iteration 0, loss = 5438.9648\n",
      "Validation loss = 6421.8682\n",
      "Iteration 0, loss = 5432.9272\n",
      "Validation loss = 6419.4346\n",
      "Iteration 0, loss = 5427.5747\n",
      "Validation loss = 6417.3945\n",
      "Iteration 0, loss = 5422.7300\n",
      "Validation loss = 6415.6460\n",
      "Iteration 0, loss = 5418.2695\n",
      "Validation loss = 6414.1250\n",
      "Iteration 0, loss = 5414.1021\n",
      "Validation loss = 6412.7793\n",
      "Iteration 0, loss = 5410.1665\n",
      "Validation loss = 6411.5771\n",
      "Iteration 0, loss = 5406.4204\n",
      "Validation loss = 6410.4912\n",
      "Iteration 0, loss = 5402.8301\n",
      "Validation loss = 6409.5049\n",
      "Iteration 0, loss = 5399.3809\n",
      "Validation loss = 6408.6025\n",
      "Iteration 0, loss = 5396.0576\n",
      "Validation loss = 6407.7729\n",
      "Iteration 0, loss = 5392.8516\n",
      "Validation loss = 6407.0083\n",
      "Iteration 0, loss = 5389.7593\n",
      "Validation loss = 6406.2998\n",
      "Iteration 0, loss = 5386.7710\n",
      "Validation loss = 6405.6406\n",
      "Iteration 0, loss = 5383.8877\n",
      "Validation loss = 6405.0278\n",
      "Iteration 0, loss = 5381.1050\n",
      "Validation loss = 6404.4551\n",
      "Iteration 0, loss = 5378.4199\n",
      "Validation loss = 6403.9185\n",
      "Iteration 0, loss = 5375.8320\n",
      "Validation loss = 6403.4131\n",
      "Iteration 0, loss = 5373.3340\n",
      "Validation loss = 6402.9370\n",
      "Iteration 0, loss = 5370.9277\n",
      "Validation loss = 6402.4873\n",
      "Iteration 0, loss = 5368.6074\n",
      "Validation loss = 6402.0610\n",
      "Iteration 0, loss = 5366.3706\n",
      "Validation loss = 6401.6572\n",
      "Iteration 0, loss = 5364.2144\n",
      "Validation loss = 6401.2720\n",
      "Iteration 0, loss = 5362.1362\n",
      "Validation loss = 6400.9053\n",
      "Iteration 0, loss = 5360.1323\n",
      "Validation loss = 6400.5566\n",
      "Iteration 0, loss = 5358.2007\n",
      "Validation loss = 6400.2217\n",
      "Iteration 0, loss = 5356.3364\n",
      "Validation loss = 6399.9023\n",
      "Iteration 0, loss = 5354.5366\n",
      "Validation loss = 6399.5957\n",
      "Iteration 0, loss = 5352.8008\n",
      "Validation loss = 6399.3022\n",
      "Iteration 0, loss = 5351.1235\n",
      "Validation loss = 6399.0220\n",
      "Iteration 0, loss = 5349.5020\n",
      "Validation loss = 6398.7520\n",
      "Iteration 0, loss = 5347.9370\n",
      "Validation loss = 6398.4941\n",
      "Iteration 0, loss = 5346.4214\n",
      "Validation loss = 6398.2466\n",
      "Iteration 0, loss = 5344.9546\n",
      "Validation loss = 6398.0088\n",
      "Iteration 0, loss = 5343.5356\n",
      "Validation loss = 6397.7822\n",
      "Iteration 0, loss = 5342.1602\n",
      "Validation loss = 6397.5645\n",
      "Iteration 0, loss = 5340.8267\n",
      "Validation loss = 6397.3569\n",
      "Iteration 0, loss = 5339.5342\n",
      "Validation loss = 6397.1587\n",
      "Iteration 0, loss = 5338.2778\n",
      "Validation loss = 6396.9678\n",
      "Iteration 0, loss = 5337.0591\n",
      "Validation loss = 6396.7856\n",
      "Iteration 0, loss = 5335.8745\n",
      "Validation loss = 6396.6133\n",
      "Iteration 0, loss = 5334.7241\n",
      "Validation loss = 6396.4487\n",
      "Iteration 0, loss = 5333.6045\n",
      "Validation loss = 6396.2905\n",
      "Iteration 0, loss = 5332.5151\n",
      "Validation loss = 6396.1421\n",
      "Iteration 0, loss = 5331.4541\n",
      "Validation loss = 6395.9995\n",
      "Iteration 0, loss = 5330.4194\n",
      "Validation loss = 6395.8643\n",
      "Iteration 0, loss = 5329.4136\n",
      "Validation loss = 6395.7358\n",
      "Iteration 0, loss = 5328.4302\n",
      "Validation loss = 6395.6152\n",
      "Iteration 0, loss = 5327.4722\n",
      "Validation loss = 6395.5005\n",
      "Iteration 0, loss = 5326.5356\n",
      "Validation loss = 6395.3921\n",
      "Iteration 0, loss = 5325.6221\n",
      "Validation loss = 6395.2896\n",
      "Iteration 0, loss = 5324.7305\n",
      "Validation loss = 6395.1953\n",
      "Iteration 0, loss = 5323.8574\n",
      "Validation loss = 6395.1040\n",
      "Iteration 0, loss = 5323.0039\n",
      "Validation loss = 6395.0186\n",
      "Iteration 0, loss = 5322.1699\n",
      "Validation loss = 6394.9404\n",
      "Iteration 0, loss = 5321.3525\n",
      "Validation loss = 6394.8652\n",
      "Iteration 0, loss = 5320.5532\n",
      "Validation loss = 6394.7969\n",
      "Iteration 0, loss = 5319.7705\n",
      "Validation loss = 6394.7324\n",
      "Iteration 0, loss = 5319.0034\n",
      "Validation loss = 6394.6729\n",
      "Iteration 0, loss = 5318.2520\n",
      "Validation loss = 6394.6182\n",
      "Iteration 0, loss = 5317.5151\n",
      "Validation loss = 6394.5664\n",
      "Iteration 0, loss = 5316.7925\n",
      "Validation loss = 6394.5205\n",
      "Iteration 0, loss = 5316.0825\n",
      "Validation loss = 6394.4775\n",
      "Iteration 0, loss = 5315.3872\n",
      "Validation loss = 6394.4395\n",
      "Iteration 0, loss = 5314.7046\n",
      "Validation loss = 6394.4043\n",
      "Iteration 0, loss = 5314.0342\n",
      "Validation loss = 6394.3730\n",
      "Iteration 0, loss = 5313.3755\n",
      "Validation loss = 6394.3447\n",
      "Iteration 0, loss = 5312.7290\n",
      "Validation loss = 6394.3208\n",
      "Iteration 0, loss = 5312.0918\n",
      "Validation loss = 6394.2983\n",
      "Iteration 0, loss = 5311.4678\n",
      "Validation loss = 6394.2803\n",
      "Iteration 0, loss = 5310.8540\n",
      "Validation loss = 6394.2646\n",
      "Iteration 0, loss = 5310.2480\n",
      "Validation loss = 6394.2510\n",
      "Iteration 0, loss = 5309.6538\n",
      "Validation loss = 6394.2407\n",
      "Iteration 0, loss = 5309.0698\n",
      "Validation loss = 6394.2329\n",
      "Iteration 0, loss = 5308.4941\n",
      "Validation loss = 6394.2271\n",
      "Iteration 0, loss = 5307.9268\n",
      "Validation loss = 6394.2241\n",
      "Iteration 0, loss = 5307.3682\n",
      "Validation loss = 6394.2217\n",
      "Iteration 0, loss = 5306.8188\n",
      "Validation loss = 6394.2217\n",
      "Iteration 0, loss = 5306.2769\n",
      "Validation loss = 6394.2241\n",
      "Iteration 0, loss = 5305.7427\n",
      "Validation loss = 6394.2290\n",
      "Iteration 0, loss = 5305.2168\n",
      "Validation loss = 6394.2329\n",
      "Iteration 0, loss = 5304.6978\n",
      "Validation loss = 6394.2407\n",
      "Iteration 0, loss = 5304.1855\n",
      "Validation loss = 6394.2480\n",
      "Iteration 0, loss = 5303.6831\n",
      "Validation loss = 6394.2588\n",
      "Iteration 0, loss = 5303.1846\n",
      "Validation loss = 6394.2686\n",
      "Iteration 0, loss = 5302.6934\n",
      "Validation loss = 6394.2803\n",
      "Iteration 0, loss = 5302.2090\n",
      "Validation loss = 6394.2930\n",
      "Iteration 0, loss = 5301.7305\n",
      "Validation loss = 6394.3066\n"
     ]
    }
   ],
   "source": [
    "model= Three_Layer_Network(no_input_features)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=2e-5)\n",
    "train_model(model, optimizer, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import vstack\n",
    "from numpy import sqrt\n",
    "from pandas import read_csv\n",
    "mse, r2 = evaluate_model(loader_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "no_input_features = X_train_np.shape[1]\n",
    "class Five_Layer_Network(torch.nn.Module):\n",
    "    def __init__(self,no_input_features):\n",
    "        super(Three_Layer_Network,self).__init__()\n",
    "        self.layer1=nn.Linear(no_input_features, 30)\n",
    "        nn.init.xavier_uniform_(self.layer1.weight)\n",
    "        self.layer2=nn.ReLU()\n",
    "        self.layer3=nn.Linear(30,10)\n",
    "        nn.init.xavier_uniform_(self.layer3.weight)\n",
    "        self.layer4=nn.ReLU()\n",
    "        self.layer5=nn.Linear(10,8)\n",
    "        nn.init.xavier_uniform_(self.layer5.weight)\n",
    "        self.layer6=nn.ReLU()\n",
    "        self.layer7=nn.Linear(8,1)\n",
    "        nn.init.xavier_uniform_(self.layer7.weight)\n",
    "    def forward(self,x):\n",
    "       y_predicted=self.layer7(self.layer6(self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x)))))))\n",
    "       return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Three_Layer_Network(no_input_features)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=2e-5)\n",
    "train_model(model, optimizer, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import vstack\n",
    "from numpy import sqrt\n",
    "from pandas import read_csv\n",
    "mse, r2 = evaluate_model(loader_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
